{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb141f4-4d84-4bcf-838f-741d50eda86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark Context\n",
    "SparkContext is the core entry point to Spark.\n",
    "\n",
    "It:\n",
    "- Connects your application to the Spark cluster\n",
    "- Talks to the Cluster Manager (Standalone / YARN / Kubernetes)\n",
    "- Manages executors, memory, and task scheduling\n",
    "- Is required to create RDDs\n",
    "\n",
    "Key points:\n",
    "- Low-level API (older style)\n",
    "- Works mainly with RDDs\n",
    "- You can have only ONE SparkContext per JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23a17b97-bf2a-4dfe-b7d5-cbc444b74d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee21863-761c-4df8-b34b-970159f90d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark Session\n",
    "SparkSession is a unified, higher-level entry point introduced in Spark 2.0.\n",
    "\n",
    "It wraps and manages:\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "So you don’t need to create them separately.\n",
    "\n",
    "What it provides:\n",
    "- DataFrame API\n",
    "- SQL queries\n",
    "- Table access\n",
    "- Streaming\n",
    "- ML integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a199a502-e378-4e1c-932b-7b15200a79ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e87d66-e3ad-48d6-9fea-dd8dc1061387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark Session in detail\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f219a4-0950-4303-9b63-f23e44113798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Builder\n",
    "builder is a configuration object (a factory) used to configure SparkSession before it is created.\n",
    "\n",
    "It says: “I’m defining how my Spark application should be set up.”\n",
    "\n",
    "Internally\n",
    "- It collects all configs (app name, master, memory, extensions, etc.)\n",
    "- Does not start Spark yet\n",
    "- Just prepares settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c0e0a2-97de-4567-a26b-e0c8db1eca5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Application Name\n",
    "Sets the Spark application name.\n",
    ".appName(\"MyApp\")\n",
    "\n",
    "Why this matters\n",
    "Appears in:\n",
    "- Spark UI\n",
    "- Databricks Jobs UI\n",
    "- Cluster event logs\n",
    "\n",
    "Helps identify which job/app is running\n",
    "\n",
    "Important notes\n",
    "- Does NOT affect performance\n",
    "- Purely for identification & monitoring\n",
    "\n",
    "In Databricks, this name may be overridden by:\n",
    "- Notebook name\n",
    "- Job name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceb9d4a3-ad44-42f4-9353-6f3a30f8d584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####getorcreate\n",
    "This is the MOST important part\n",
    "\n",
    "What it does:\n",
    "\n",
    "IF SparkSession already exists:\n",
    "    return the existing one\n",
    "ELSE:\n",
    "    create a new SparkSession\n",
    "\n",
    "Why this is critical\n",
    "- Spark allows only ONE SparkContext per JVM\n",
    "- Creating multiple SparkContexts will crash the app\n",
    "- getOrCreate() prevents this problem safely\n",
    "\n",
    "**How get or create works internally:**\n",
    "\n",
    "Step 1: Check existing session:\n",
    "Is there already an active SparkSession?\n",
    "\n",
    "**Step 2: Create SparkContext (if needed):**\n",
    "\n",
    "If no SparkContext exists:\n",
    "- Connects to cluster manager\n",
    "- Starts driver\n",
    "- Requests executors\n",
    "\n",
    "**Step 3: Create SparkSession:**\n",
    "\n",
    "Wraps the SparkContext\n",
    "Initializes:\n",
    "- SQL engine (Catalyst)\n",
    "- Optimizer\n",
    "- Catalog\n",
    "- Session configs:Sets up memory & cores"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_session",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
