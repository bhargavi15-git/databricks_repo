{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c92469f-8114-4f71-9dfe-ba7e566b8b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark.read:\n",
    "File types:\n",
    "- jdbc\n",
    "- csv\n",
    "- orc\n",
    "- parquet\n",
    "- table\n",
    "- text\n",
    "- xml\n",
    "- json\n",
    "\n",
    "Other options:\n",
    "- option\n",
    "- options\n",
    "- schema\n",
    "- load\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd0c417-5afd-4cc8-a7b5-654a662575c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.csv\n",
    "\n",
    "- > header,inferschema,sampling ratio,sep,linesep,\n",
    "- > quote,escape,\n",
    "- > nullvalue,emptyvalue,\n",
    "- > mode,columnofcorruptrecord,\n",
    "- > comment,ignoreLeadingWhiteSpace,ignoreTrailingWhiteSpace\n",
    "- > dataformat,timestampformat\n",
    "- > maxcolumns\n",
    "- > multiline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Important options:\n",
    "- **header**-> By default False. Set to True when the first row of data has to be considered as header **option(\"header\",\"True\")**\n",
    "- **inferschema**-> without this all columns by default are considers as string. Infers the schema by scanning the whole fail(not recommaned for large files) **option(\"inferSchema\",\"True\")**\n",
    "- **sampling ratio**-> this helps inferschema, instead of scanning the whole file it scans only respective percentage **option(\"inferSchema\",\"True\").option(\"samplingRatio\",0.1)**\n",
    "- **sep** -> Column Seperator, default is ',' option(\"sep\",\"|\")\n",
    "- **linesep** -> default new line option(\"lineSep\",\"|\")\n",
    "- **quote and escape** ->\n",
    "  csv:\n",
    "  id,name,description\n",
    "\n",
    "  1,Alice,\"Senior \\\"Data Engineer\\\", Spark expert\"\n",
    "\n",
    "  spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\    #quote is used as , is present inside a value\n",
    "\n",
    "    .option(\"escape\", \"\\\\\") \\   #escape is used as quote is in quote\n",
    "\n",
    "    .csv(path)\n",
    "\n",
    "    ‚úî Use quote when text contains delimiter or newline\n",
    "    ‚úî Use escape when text contains quote character itself\n",
    "\n",
    "- **null value and emptyvalue**\n",
    "    - By default any empty value is treated as null\n",
    "    - when an string comes as 'null' it is treated as mere string\n",
    "    - the value given in nullvalue is also treated as null along with empty fields\n",
    "\n",
    "    ‚úî These options apply globally to all columns while reading the CSV.\n",
    "    ‚úî Empty CSV field ‚Üí NULL (by default)\n",
    "    ‚úî nullValue handles explicit tokens like NA, NULL\n",
    "    ‚úî emptyValue replaces empty fields with a value\n",
    "\n",
    "    example csv:\n",
    "    id,value\n",
    "    1,\n",
    "    2,null\n",
    "    3,NA\n",
    "    4,100\n",
    "\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"nullValue\", \"null\") \\  #when we dont specify it treats null as string\n",
    "\n",
    "        .option(\"nullValue\", \"NA\") \\\n",
    "        .csv(\"/path/file.csv\")\n",
    "      \n",
    "    with empty value:\n",
    "\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"emptyValue\",\"0\") \\  #now the emty value will be filled with 0\n",
    "\n",
    "        .option(\"nullValue\", \"null\") \\  #when we dont specify it treats null as string\n",
    "        \n",
    "        .option(\"nullValue\", \"NA\") \\\n",
    "        .csv(\"/path/file.csv\")\n",
    "\n",
    "- **mode**-> Defines behavior when parsing errors occur.\n",
    "\n",
    "  | Mode                   | Behavior          |\n",
    "  | ---------------------- | ----------------- |\n",
    "  | `PERMISSIVE` (default) | Bad rows ‚Üí `null` |\n",
    "  | `DROPMALFORMED`        | Drops bad rows    |\n",
    "  | `FAILFAST`             | Fails immediately |\n",
    "\n",
    "\n",
    "- ** columnNameOfCorruptRecord** -> Stores bad rows in a separate column.\n",
    "\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt\")\n",
    "\n",
    "  ‚úî Useful for debugging dirty data\n",
    "\n",
    "- **comment** -> Ignores lines starting with a character.\n",
    "\n",
    "    .option(\"comment\", \"@\")\n",
    "\n",
    "    Example:\n",
    "    - 1,Kavi,30\n",
    "    - @this is a comment\n",
    "    - 2,Alice,25\n",
    "\n",
    "- **ignoreLeadingWhiteSpace / ignoreTrailingWhiteSpace**\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    ‚úî Cleans extra spaces\n",
    "\n",
    "- **dateFormat / timestampFormat**\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ‚úî Needed when parsing dates\n",
    "\n",
    "- **maxColumns** -> Limits maximum number of columns.\n",
    "\n",
    "    .option(\"maxColumns\", \"20480\")\n",
    "\n",
    "    Prevents malformed wide files from crashing Spark\n",
    "\n",
    "- **multiline** -> Allows rows to expant to multi line\n",
    "\n",
    "    without multiline the below code is considered as 2 records:\n",
    "\n",
    "    1,John,\"Hello\n",
    "    \n",
    "    How are you?\"\n",
    "\n",
    "    with multiline=True spark will be able to understand that these are same rows with quotes\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3be8cf-6130-41b4-91ce-566b022780e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####spark.read.orc and spark.read.\n",
    "\n",
    "‚ö†Ô∏è Important upfront\n",
    "There are very few true reader options. ORC andd Parquet are self-describing, so most behavior is controlled by Spark configs, not .option().\n",
    "\n",
    "- **mergeSchema**(common for orc and parquet)\n",
    "    -     .option(\"mergeSchema\", \"true\")\n",
    "    -     Merge schemas from multiple ORC files\n",
    "    -     Default: false\n",
    "    -     Costly (reads metadata of all files)\n",
    "- **pathGlobFilter**(common for orc and parquet)\n",
    "    -       .option(\"pathGlobFilter\", \"*.orc\")\n",
    "    -       Read only files matching the pattern\n",
    "    -       Useful when directory has mixed files\n",
    "\n",
    "- **recursiveFileLookup**(common for orc and parquet)\n",
    "    -     .option(\"recursiveFileLookup\", \"true\")\n",
    "    -     Recursively read ORC files from subdirectories\n",
    "    -     ‚ùå Not for Hive-style partitioned data\n",
    "\n",
    "- **basePath**(common for orc and parquet)\n",
    "  -     .option(\"basePath\", \"/data/sales\")\n",
    "  -     Required when reading partitioned data using wildcards\n",
    "  -     Helps Spark correctly infer partition columns\n",
    "\n",
    "- **schema**(common for orc and parquet)\n",
    "    -     .schema(custom_schema)\n",
    "    -     Explicitly provide schema\n",
    "    -     Rarely needed (ORC already stores schema)\n",
    "\n",
    "- **datetimeRebaseMode**(common for orc and parquet)\n",
    "    -     .option(\"datetimeRebaseMode\", \"LEGACY\")\n",
    "    -     For legacy ORC files written by old Spark/Hive\n",
    "    -     Values:\n",
    "      - CORRECTED (default)\n",
    "      - LEGACY\n",
    "- **int96RebaseMode**(Parquet specific)\n",
    "    -     .option(\"int96RebaseMode\", \"LEGACY\")\n",
    "    -     Handles legacy INT96 timestamp columns\n",
    "    -    This option is Parquet-specific (ORC does not have INT96).\n",
    "    -     Values:\n",
    "            -  CORRECTED (default)\n",
    "            -  LEGACY\n",
    "‚öôÔ∏è ORC behavior controlled via Spark configs (NOT .option())\n",
    "\n",
    "These are important but separate:\n",
    "\n",
    "- spark.sql.orc.filterPushdown\n",
    "- spark.sql.orc.enableVectorizedReader\n",
    "- spark.sql.orc.mergeSchema\n",
    "\n",
    "‚öôÔ∏è Parquet behavior controlled via Spark configs (NOT .option())\n",
    "\n",
    "These are very important in real projects:\n",
    "\n",
    "- spark.sql.parquet.filterPushdown\n",
    "- spark.sql.parquet.enableVectorizedReader\n",
    "- spark.sql.parquet.mergeSchema\n",
    "- spark.sql.parquet.binaryAsString\n",
    "- spark.sql.parquet.int96AsTimestamp\n",
    "\n",
    "Example:\n",
    "spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60884439-81ac-4c69-b05c-05796c12ca88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.table\n",
    "spark.read.table() takes ONLY a table name as input and returns a DataFrame using table metadata ‚Äî no options, no paths, no format needed.\n",
    "\n",
    "These are equivalent:\n",
    "\n",
    "- spark.read.table(\"sales.orders\")\n",
    "- spark.sql(\"SELECT * FROM sales.orders\")\n",
    "\n",
    "Difference:\n",
    "- table() ‚Üí DataFrame API\n",
    "- sql() ‚Üí SQL string\n",
    "\n",
    "df = (\n",
    "    spark.read.table(\"main.finance.transactions\")\n",
    "         .filter(\"txn_date >= '2024-01-01'\")\n",
    "         .select(\"txn_id\", \"amount\")\n",
    ")\n",
    "\n",
    "- ‚úî Governance\n",
    "- ‚úî Unity Catalog permissions\n",
    "- ‚úî Lineage tracking\n",
    "\n",
    "- What happens internally (important)\n",
    "    -       When you run: -> spark.read.table(\"sales.orders\")\n",
    "    -       Spark:\n",
    "      - Looks up the table in catalog\n",
    "      - Reads table metadata\n",
    "      - Finds:\n",
    "          - Storage location\n",
    "          - File format (Delta / Parquet / ORC)\n",
    "          - Schema\n",
    "          - Partitions\n",
    "          - Uses the correct reader automatically\n",
    "          - üëâ You do NOT specify format or path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f222e05-9b8d-4667-956b-d0f8fd14f5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.text\n",
    "\n",
    "      df = (\n",
    "          spark.read\n",
    "              .option(\"wholetext\", \"false\")\n",
    "              .option(\"lineSep\", \"\\n\")\n",
    "              .option(\"pathGlobFilter\", \"*.txt\")\n",
    "              .option(\"recursiveFileLookup\", \"false\")\n",
    "              .option(\"encoding\", \"UTF-8\")\n",
    "              .text(\"/path/to/text/files\")\n",
    "      )\n",
    "\n",
    "\n",
    "üîπ ALL valid spark.read.text() options\n",
    "- **wholetext**\n",
    "    -   .option(\"wholetext\", \"true\"\n",
    "    -   Entire file becomes ONE row\n",
    "    -   Default: false\n",
    "\n",
    "- **lineSep**\n",
    "    - .option(\"lineSep\", \"\\n\")\n",
    "    - Custom line separator\n",
    "    - Default: \\n\n",
    "\n",
    "- **pathGlobFilter**\n",
    "    - .option(\"pathGlobFilter\", \"*.log\")\n",
    "\n",
    "- **recursiveFileLookup**\n",
    "    - .option(\"recursiveFileLookup\", \"true\")\n",
    "\n",
    "- **encoding**\n",
    "    - .option(\"encoding\", \"UTF-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39aac1b6-b2e0-4a13-83be-e69e77d0d8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####spark.read.jdbc\n",
    "\n",
    "Reads data from a relational database (MySQL, Postgres, Oracle, SQL Server, etc.) using JDBC and returns a DataFrame.\n",
    "\n",
    "- **IN (Inputs)**\n",
    "      ‚úÖ Required parameters (minimum)\n",
    "\n",
    "      spark.read.jdbc(\n",
    "          url=\"jdbc:mysql://host:3306/db\",\n",
    "          table=\"orders\",\n",
    "          properties={\n",
    "              \"user\": \"username\",\n",
    "              \"password\": \"password\",\n",
    "              \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "          }\n",
    "      )\n",
    "- **Function signatures (important)**\n",
    "    - **Table-based read**\n",
    "          jdbc(url, table, properties)\n",
    "    - **Query-based read**\n",
    "          jdbc(url, \"(select * from orders) t\", properties)\n",
    "          ‚ö†Ô∏è Query MUST be aliased.\n",
    "- **OUT (Output)**\n",
    "      -   A DataFrame\n",
    "      -   Schema inferred from DB metadata\n",
    "      -   Rows fetched via JDBC\n",
    "\n",
    "- **ALL valid spark.read.jdbc() options**\n",
    "\n",
    "  These can be passed via .option() or properties.\n",
    "\n",
    "    **Connection options**\n",
    "          -   Option\tMeaning\n",
    "          -   url\tJDBC URL\n",
    "          -   dbtable\tTable name or subquery\n",
    "          -   user\tDB username\n",
    "          -   password\tDB password\n",
    "          -   driver\tJDBC driver class\n",
    "\n",
    "    **Parallel read (VERY IMPORTANT)**\n",
    "            -     Option\tPurpose\n",
    "            -     partitionColumn\tColumn to split data\n",
    "            -     lowerBound\tMin value\n",
    "            -     upperBound\tMax value\n",
    "            -     numPartitions\tParallel connections\n",
    "\n",
    "    Example:\n",
    "        spark.read \\\n",
    "          .option(\"url\", url) \\\n",
    "          .option(\"dbtable\", \"orders\") \\\n",
    "          .option(\"partitionColumn\", \"id\") \\\n",
    "          .option(\"lowerBound\", 1) \\\n",
    "          .option(\"upperBound\", 100000) \\\n",
    "          .option(\"numPartitions\", 10) \\\n",
    "          .load()\n",
    "       **Alternative partitioning**\n",
    "\n",
    "        -     predicates\tList of WHERE clauses\n",
    "        -   spark.read.jdbc(url, \"orders\", predicates, properties)\n",
    "\n",
    "            predicates = [\n",
    "                \"country = 'US'\",\n",
    "                \"country = 'IN'\",\n",
    "                \"country = 'UK'\"\n",
    "            ]\n",
    "\n",
    "        **Fetching & performance**\n",
    "    \n",
    "          -     fetchsize\tRows per DB fetch\n",
    "          -     batchsize\tWrite-side mostly\n",
    "          -     queryTimeout\tSeconds\n",
    "\n",
    "        **Schema & types**\n",
    "    \n",
    "          -     customSchema\tOverride column types\n",
    "          -     pushDownPredicate\tPush filters to DB\n",
    "\n",
    "        **Security**\n",
    "    \n",
    "          -     ssl\tEnable SSL\n",
    "          -     sessionInitStatement\tInit SQL\n",
    "\n",
    "- **template (BEST PRACTICE)**\n",
    "\n",
    "      jdbc_url = \"jdbc:mysql://host:3306/sales\"\n",
    "      props = {\n",
    "          \"user\": \"user\",\n",
    "          \"password\": \"password\",\n",
    "          \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "      }\n",
    "\n",
    "      df = (\n",
    "          spark.read\n",
    "              .option(\"url\", jdbc_url)\n",
    "              .option(\"dbtable\", \"orders\")\n",
    "              .option(\"partitionColumn\", \"id\")\n",
    "              .option(\"lowerBound\", \"1\")\n",
    "              .option(\"upperBound\", \"100000\")\n",
    "              .option(\"numPartitions\", \"8\")\n",
    "              .option(\"fetchsize\", \"1000\")\n",
    "              .load()\n",
    ")\n",
    "- **Common mistakes ‚ùå**\n",
    "      - ‚ùå Not using partitioning ‚Üí single-threaded read\n",
    "      - ‚ùå Using non-numeric partitionColumn\n",
    "      - ‚ùå Forgetting alias in subquery\n",
    "      - ‚ùå Pulling huge tables without filters\n",
    "- **When to use JDBC**\n",
    "      - ‚úÖ Small‚Äìmedium tables\n",
    "      - ‚úÖ Reference / dimension data\n",
    "      - ‚ùå Massive fact tables (prefer dumps to Parquet)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_read",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
