{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c92469f-8114-4f71-9dfe-ba7e566b8b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark.read:\n",
    "File types:\n",
    "- jdbc\n",
    "- csv\n",
    "- orc\n",
    "- parquet\n",
    "- table\n",
    "- text\n",
    "- xml\n",
    "- json\n",
    "\n",
    "Other options:\n",
    "- option\n",
    "- options\n",
    "- schema\n",
    "- load\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd0c417-5afd-4cc8-a7b5-654a662575c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.csv\n",
    "\n",
    "**Very Important:** path, schema, sep, header, inferSchema, samplingRatio, mode(permissive,dropMalformed,Failfast), columnNameOfCorruptRecord <br>\n",
    "**Important:** quote, escape,comment, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, dateFormat, timestampFormat, multiLine, enforceSchema,pathGlobFilter, recursiveFileLookup,modifiedBefore,modifiedAfter,encoding,  maxColumns, maxCharsPerColumn,  emptyValue,  lineSep \n",
    "\n",
    "\n",
    "Important options:\n",
    "- **header**-> By default False. Set to True when the first row of data has to be considered as header **option(\"header\",\"True\")**\n",
    "- **inferschema**-> without this all columns by default are considers as string. Infers the schema by scanning the whole file(not recommaned for large files) **option(\"inferSchema\",\"True\")**\n",
    "- **sampling ratio**-> this helps inferschema, instead of scanning the whole file it scans only respective percentage **option(\"inferSchema\",\"True\").option(\"samplingRatio\",0.1)**\n",
    "- **sep** -> Column Seperator, default is ',' option(\"sep\",\"|\")\n",
    "- **linesep** -> default new line option(\"lineSep\",\"|\")\n",
    "- **quote and escape** ->\n",
    "  csv:\n",
    "  id,name,description\n",
    "\n",
    "  1,Alice,\"Senior \\\"Data Engineer\\\", Spark expert\"\n",
    "\n",
    "  spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\    #quote is used as , is present inside a value\n",
    "\n",
    "    .option(\"escape\", \"\\\\\") \\   #escape is used as quote is in quote\n",
    "\n",
    "    .csv(path)\n",
    "\n",
    "    ‚úî Use quote when text contains delimiter or newline\n",
    "    ‚úî Use escape when text contains quote character itself\n",
    "\n",
    "- **recursiveFileLookup and pathGlobalFilter:**\n",
    "        -     we usally scan a file in spark while read. To read a folder and to scan all files and subfolders'files in it we use recurisveFileLookup. \n",
    "        -     PathGlobalfilter helps when we have different file types in it. It helps to specify a pattern by which the file search has to happen.\n",
    "\n",
    "- **modifiedBefore and modifiedAfter**\n",
    "        - It is Used along with rescursiveFileLookup to search based on file's modified date. The file will be picked based on the modified dates given\n",
    "\n",
    "- **null value and emptyvalue and nanValue**\n",
    "    - By default any empty value is treated as null, any numeric or date column coming as null is treated as null\n",
    "    - anymismtach in schema and incoming value is treated as null\n",
    "    - when an string comes as 'null' it is treated as mere string, nut term 'null' in numeric column(int,double,float)\n",
    "    - the value given in nullvalue option is also treated as null along with empty fields\n",
    "\n",
    "    ‚úî These options apply globally to all columns while reading the CSV.\n",
    "    ‚úî Empty CSV field ‚Üí NULL (by default)\n",
    "    ‚úî nullValue handles explicit tokens like NA, NULL\n",
    "    ‚úî emptyValue replaces empty fields with a value\n",
    "\n",
    "    example csv:\n",
    "    id,value\n",
    "    1,\n",
    "    2,null\n",
    "    3,NA\n",
    "    4,100\n",
    "\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"nullValue\", \"null\") \\  #when we dont specify it treats null as string\n",
    "\n",
    "        .option(\"nullValue\", \"NA\") \\\n",
    "        .csv(\"/path/file.csv\")\n",
    "      \n",
    "    with empty value:\n",
    "\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"emptyValue\",\"0\") \\  #now the emty value will be filled with 0\n",
    "\n",
    "        .option(\"nullValue\", \"null\") \\  #when we dont specify it treats null as string\n",
    "        \n",
    "        .option(\"nullValue\", \"NA\") \\\n",
    "        .csv(\"/path/file.csv\")\n",
    "\n",
    "        nanValue works ONLY for FloatType / DoubleType:\n",
    "            If we have any value we want to mention it as Nan value we can use this function:\n",
    "            eg: .option(nanValue='-1')\n",
    "            custid, salary\n",
    "            1,10.0\n",
    "            2,-1\n",
    "\n",
    "| Scenario                | emptyValue works?    |\n",
    "| ----------------------- | -------------------- |\n",
    "| No schema (all strings) | ‚úÖ YES                |\n",
    "| inferSchema             | ‚ö†Ô∏è sometimes         |\n",
    "| Explicit schema         | ‚ùå NOT for StringType |\n",
    "\n",
    "\n",
    "\n",
    "- **mode**-> Defines behavior when parsing errors occur.\n",
    "\n",
    "  | Mode                   | Behavior          |\n",
    "  | ---------------------- | ----------------- |\n",
    "  | `PERMISSIVE` (default) | Bad column values ‚Üí `null` |\n",
    "  | `DROPMALFORMED`        | Drops bad rows    |\n",
    "  | `FAILFAST`             | Fails immediately |\n",
    "\n",
    "\n",
    "- **columnNameOfCorruptRecord** -> Stores bad rows in a separate column.\n",
    "\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt\")\n",
    "\n",
    "  ‚úî Useful for debugging dirty data\n",
    "\n",
    "- **comment** -> Ignores lines starting with a character.\n",
    "\n",
    "    .option(\"comment\", \"@\")\n",
    "\n",
    "    Example:\n",
    "    - 1,Kavi,30\n",
    "    - @this is a comment\n",
    "    - 2,Alice,25\n",
    "\n",
    "- **ignoreLeadingWhiteSpace / ignoreTrailingWhiteSpace**\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    ‚úî Cleans extra spaces\n",
    "\n",
    "- **dateFormat / timestampFormat**\n",
    "        By default spark expects 'yyyy-MM-dd' and 'yyyy-MM-dd HH:mm:ss' while reading  \n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ‚úî Needed when parsing dates\n",
    "\n",
    "- **maxColumns / maxCharsPerColumn** -> Limits maximum number of columns and maximum number of characters per column\n",
    "\n",
    "    .option(\"maxColumns\", \"20480\")\n",
    "\n",
    "    Prevents malformed wide files from crashing Spark\n",
    "\n",
    "- **multiline** -> Allows rows to expant to multi line(It helps only when one column value is expand to next line)\n",
    "\n",
    "    -     without multiline the below code is considered as 2 records:\n",
    "\n",
    "    1,John,\"Hello\n",
    "    \n",
    "    How are you?\"\n",
    "\n",
    "    -     with multiline=True spark will be able to understand that these are same rows with quotes\n",
    "\n",
    "    -     Important Note: Multiline does work in the below scenario:\n",
    "\n",
    "    1,John,\n",
    "\n",
    "    \"Hello How are you?\"\n",
    "\n",
    "- **enforceSchema:**\n",
    "\n",
    "    - Spark forces each column to match the schema you provided. By default it is true.\n",
    "    - If a value cannot be converted, Spark sets it to NULL\n",
    "    - Spark does NOT infer types even if the value looks different\n",
    "    - **When enforceSchema = false ‚ö†Ô∏è**\n",
    "\n",
    "                -             Spark tries to be lenient\n",
    "                -             It may silently accept invalid values\n",
    "                -             Can cause unexpected behavior\n",
    "\n",
    "- **encoding:** - To read special characters and different language from csv files.default(encoding = UTF-8)\n",
    "\n",
    "        -----------------------------------------------\n",
    "        | Encoding       | When used                  |\n",
    "        | -------------- | -------------------------- |\n",
    "        | `UTF-8`        | Default, most modern files |\n",
    "        | `ISO-8859-1`   | Older European systems     |\n",
    "        | `UTF-16`       | Excel-exported files       |\n",
    "        | `Windows-1252` | Windows CSV files(Windows-1252 (common in Excel CSV)superset of ISO-8859-1)|\n",
    "        -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f222e05-9b8d-4667-956b-d0f8fd14f5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.text\n",
    "\n",
    "      df = (\n",
    "          spark.read\n",
    "              .option(\"wholetext\", \"false\")\n",
    "              .option(\"lineSep\", \"\\n\")\n",
    "              .option(\"pathGlobFilter\", \"*.txt\")\n",
    "              .option(\"recursiveFileLookup\", \"false\")\n",
    "              .option(\"encoding\", \"UTF-8\")\n",
    "              .text(\"/path/to/text/files\")\n",
    "      )\n",
    "\n",
    "\n",
    "üîπ ALL valid spark.read.text() options\n",
    "- **wholetext**\n",
    "    -   .option(\"wholetext\", \"true\"\n",
    "    -   Entire file becomes ONE row\n",
    "    -   Default: false\n",
    "    -   If true, read each file from input path(s) as a single row.\n",
    "\n",
    "- **lineSep**\n",
    "    - .option(\"lineSep\", \"\\n\")\n",
    "    - Custom line separator\n",
    "    - Default: \\n\n",
    "\n",
    "- **pathGlobFilter**\n",
    "    - .option(\"pathGlobFilter\", \"*.log\")\n",
    "\n",
    "- **recursiveFileLookup**\n",
    "    - .option(\"recursiveFileLookup\", \"true\")\n",
    "\n",
    "- **encoding**\n",
    "    - .option(\"encoding\", \"UTF-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44eb027b-fd3e-490a-84d2-b560428de9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####spark.read.orc\n",
    "\n",
    "- path\n",
    "- mergeSchema\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00d993c-b589-40d0-812e-bd7cc4c3f42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###spark.read.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b080cf9-12f3-4dd1-ba22-e81b806ee645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df1=spark.read.parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c980428c-b14f-479c-a85e-a33d528c2d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3be8cf-6130-41b4-91ce-566b022780e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####spark.read.orc and spark.read.parquet and spark.read.delta\n",
    "\n",
    "‚ö†Ô∏è Important upfront\n",
    "There are very few true reader options. ORC andd Parquet are self-describing, so most behavior is controlled by Spark configs, not .option().\n",
    "\n",
    "- **mergeSchema**(common for orc and parquet)\n",
    "    -     .option(\"mergeSchema\", \"true\")\n",
    "    -     Merge schemas from multiple ORC files\n",
    "    -     Default: false\n",
    "    -     Costly (reads metadata of all files)\n",
    "    -       clubs based on the column name, if two files have same column names puts them together. If 2 columns have same column but different datatypes it will throw error.\n",
    "    -       Bad for large number of small files.\n",
    "- **pathGlobFilter**(common for orc and parquet)\n",
    "    -       .option(\"pathGlobFilter\", \"*.orc\")\n",
    "    -       Read only files matching the pattern\n",
    "    -       Useful when directory has mixed files\n",
    "\n",
    "- **recursiveFileLookup**(common for orc and parquet)\n",
    "    -     .option(\"recursiveFileLookup\", \"true\")\n",
    "    -     Recursively read ORC files from subdirectories\n",
    "    -     ‚ùå Not for Hive-style partitioned data\n",
    "\n",
    "modified\n",
    "\n",
    "- **basePath**(common for orc and parquet)\n",
    "  -     .option(\"basePath\", \"/data/sales\")\n",
    "  -     Required when reading partitioned data using wildcards\n",
    "  -     Helps Spark correctly infer partition columns\n",
    "\n",
    "- **schema**(common for orc and parquet)\n",
    "    -     .schema(custom_schema)\n",
    "    -     Explicitly provide schema\n",
    "    -     Rarely needed (ORC already stores schema)\n",
    "\n",
    "- **datetimeRebaseMode**(common for orc and parquet)\n",
    "    -     .option(\"datetimeRebaseMode\", \"LEGACY\")\n",
    "    -     For legacy ORC files written by old Spark/Hive\n",
    "    -     Values:\n",
    "      - CORRECTED (default)\n",
    "      - LEGACY\n",
    "- **int96RebaseMode**(Parquet specific)\n",
    "    -     .option(\"int96RebaseMode\", \"LEGACY\")\n",
    "    -     Handles legacy INT96 timestamp columns\n",
    "    -    This option is Parquet-specific (ORC does not have INT96).\n",
    "    -     Values:\n",
    "            -  CORRECTED (default)\n",
    "            -  LEGACY\n",
    "‚öôÔ∏è ORC behavior controlled via Spark configs (NOT .option())\n",
    "\n",
    "These are important but separate:\n",
    "\n",
    "- spark.sql.orc.filterPushdown\n",
    "- spark.sql.orc.enableVectorizedReader\n",
    "- spark.sql.orc.mergeSchema\n",
    "\n",
    "‚öôÔ∏è Parquet behavior controlled via Spark configs (NOT .option())\n",
    "\n",
    "These are very important in real projects:\n",
    "\n",
    "- spark.sql.parquet.filterPushdown\n",
    "- spark.sql.parquet.enableVectorizedReader\n",
    "- spark.sql.parquet.mergeSchema\n",
    "- spark.sql.parquet.binaryAsString\n",
    "- spark.sql.parquet.int96AsTimestamp\n",
    "\n",
    "Example:\n",
    "spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60884439-81ac-4c69-b05c-05796c12ca88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Spark.read.table\n",
    "spark.read.table() takes ONLY a table name as input and returns a DataFrame using table metadata ‚Äî no options, no paths, no format needed.\n",
    "\n",
    "These are equivalent:\n",
    "\n",
    "- spark.read.table(\"sales.orders\")\n",
    "- spark.sql(\"SELECT * FROM sales.orders\")\n",
    "\n",
    "Difference:\n",
    "- table() ‚Üí DataFrame API\n",
    "- sql() ‚Üí SQL string\n",
    "\n",
    "df = (\n",
    "    spark.read.table(\"main.finance.transactions\")\n",
    "         .filter(\"txn_date >= '2024-01-01'\")\n",
    "         .select(\"txn_id\", \"amount\")\n",
    ")\n",
    "\n",
    "- ‚úî Governance\n",
    "- ‚úî Unity Catalog permissions\n",
    "- ‚úî Lineage tracking\n",
    "\n",
    "- What happens internally (important)\n",
    "    -       When you run: -> spark.read.table(\"sales.orders\")\n",
    "    -       Spark:\n",
    "      - Looks up the table in catalog\n",
    "      - Reads table metadata\n",
    "      - Finds:\n",
    "          - Storage location\n",
    "          - File format (Delta / Parquet / ORC)\n",
    "          - Schema\n",
    "          - Partitions\n",
    "          - Uses the correct reader automatically\n",
    "          - üëâ You do NOT specify format or path.\n",
    "\n",
    "\n",
    "pathGlobalFilter/modifiedBefore,modifiedAfter/recursiveLookUp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39aac1b6-b2e0-4a13-83be-e69e77d0d8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####spark.read.jdbc\n",
    "\n",
    "Reads data from a relational database (MySQL, Postgres, Oracle, SQL Server, etc.) using JDBC and returns a DataFrame.\n",
    " def jdbc(\n",
    "        self,\n",
    "        url: str,\n",
    "        table: str,\n",
    "        column: str,\n",
    "        lowerBound: Union[int, str],\n",
    "        upperBound: Union[int, str],\n",
    "        numPartitions: int,\n",
    "        *,\n",
    "        properties: Optional[Dict[str, str]] = None,\n",
    "    ) -> \"DataFrame\":\n",
    "\n",
    "- **IN (Inputs)**\n",
    "      ‚úÖ Required parameters (minimum)\n",
    "\n",
    "      spark.read.jdbc(\n",
    "          url=\"jdbc:mysql://host:3306/db\",\n",
    "          table=\"orders\",\n",
    "          properties={\n",
    "              \"user\": \"username\",\n",
    "              \"password\": \"password\",\n",
    "              \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "          }\n",
    "      )\n",
    "- **Function signatures (important)**\n",
    "    - **Table-based read**\n",
    "          jdbc(url, table, properties)\n",
    "    - **Query-based read**\n",
    "          jdbc(url, \"(select * from orders) t\", properties)\n",
    "          ‚ö†Ô∏è Query MUST be aliased.\n",
    "- **OUT (Output)**\n",
    "      -   A DataFrame\n",
    "      -   Schema inferred from DB metadata\n",
    "      -   Rows fetched via JDBC\n",
    "\n",
    "- **ALL valid spark.read.jdbc() options**\n",
    "\n",
    "  These can be passed via .option() or properties.\n",
    "\n",
    "    **Connection options**\n",
    "          -   Option\tMeaning\n",
    "          -   url\tJDBC URL\n",
    "          -   dbtable\tTable name or subquery\n",
    "          -   user\tDB username\n",
    "          -   password\tDB password\n",
    "          -   driver\tJDBC driver class\n",
    "\n",
    "    **Parallel read (VERY IMPORTANT)**\n",
    "            -     Option\tPurpose\n",
    "            -     partitionColumn\tColumn to split data\n",
    "            -     lowerBound\tMin value\n",
    "            -     upperBound\tMax value\n",
    "            -     numPartitions\tParallel connections\n",
    "\n",
    "    Example:\n",
    "        spark.read \\\n",
    "          .option(\"url\", url) \\\n",
    "          .option(\"dbtable\", \"orders\") \\\n",
    "          .option(\"partitionColumn\", \"id\") \\\n",
    "          .option(\"lowerBound\", 1) \\\n",
    "          .option(\"upperBound\", 100000) \\\n",
    "          .option(\"numPartitions\", 10) \\\n",
    "          .load()\n",
    "       **Alternative partitioning**\n",
    "\n",
    "        -     predicates\tList of WHERE clauses\n",
    "        -   spark.read.jdbc(url, \"orders\", predicates, properties)\n",
    "\n",
    "            predicates = [\n",
    "                \"country = 'US'\",\n",
    "                \"country = 'IN'\",\n",
    "                \"country = 'UK'\"\n",
    "            ]\n",
    "\n",
    "        **Fetching & performance**\n",
    "    \n",
    "          -     fetchsize\tRows per DB fetch\n",
    "          -     batchsize\tWrite-side mostly\n",
    "          -     queryTimeout\tSeconds\n",
    "\n",
    "        **Schema & types**\n",
    "    \n",
    "          -     customSchema\tOverride column types\n",
    "          -     pushDownPredicate\tPush filters to DB\n",
    "\n",
    "        **Security**\n",
    "    \n",
    "          -     ssl\tEnable SSL\n",
    "          -     sessionInitStatement\tInit SQL\n",
    "\n",
    "- **template (BEST PRACTICE)**\n",
    "\n",
    "      jdbc_url = \"jdbc:mysql://host:3306/sales\"\n",
    "      props = {\n",
    "          \"user\": \"user\",\n",
    "          \"password\": \"password\",\n",
    "          \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "      }\n",
    "\n",
    "      df = (\n",
    "          spark.read\n",
    "              .option(\"url\", jdbc_url)\n",
    "              .option(\"dbtable\", \"orders\")\n",
    "              .option(\"partitionColumn\", \"id\")\n",
    "              .option(\"lowerBound\", \"1\")\n",
    "              .option(\"upperBound\", \"100000\")\n",
    "              .option(\"numPartitions\", \"8\")\n",
    "              .option(\"fetchsize\", \"1000\")\n",
    "              .load()\n",
    ")\n",
    "- **Common mistakes ‚ùå**\n",
    "      - ‚ùå Not using partitioning ‚Üí single-threaded read\n",
    "      - ‚ùå Using non-numeric partitionColumn\n",
    "      - ‚ùå Forgetting alias in subquery\n",
    "      - ‚ùå Pulling huge tables without filters\n",
    "- **When to use JDBC**\n",
    "      - ‚úÖ Small‚Äìmedium tables\n",
    "      - ‚úÖ Reference / dimension data\n",
    "      - ‚ùå Massive fact tables (prefer dumps to Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b2451c-b6f8-405d-900d-1dedc410cbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a805340d-1427-4bf5-8126-ead7e3b175c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###spark.read.json\n",
    "\n",
    "    def json(\n",
    "        self,\n",
    "        path: Union[str, List[str], \"RDD[str]\"],\n",
    "        schema: Optional[Union[StructType, str]] = None,\n",
    "        primitivesAsString: Optional[Union[bool, str]] = None,\n",
    "        prefersDecimal: Optional[Union[bool, str]] = None,\n",
    "        allowComments: Optional[Union[bool, str]] = None,\n",
    "        allowUnquotedFieldNames: Optional[Union[bool, str]] = None,\n",
    "        allowSingleQuotes: Optional[Union[bool, str]] = None,\n",
    "        allowNumericLeadingZero: Optional[Union[bool, str]] = None,\n",
    "        allowBackslashEscapingAnyCharacter: Optional[Union[bool, str]] = None,\n",
    "        mode: Optional[str] = None,\n",
    "        columnNameOfCorruptRecord: Optional[str] = None,\n",
    "        dateFormat: Optional[str] = None,\n",
    "        timestampFormat: Optional[str] = None,\n",
    "        multiLine: Optional[Union[bool, str]] = None,\n",
    "        allowUnquotedControlChars: Optional[Union[bool, str]] = None,\n",
    "        lineSep: Optional[str] = None,\n",
    "        samplingRatio: Optional[Union[float, str]] = None,\n",
    "        dropFieldIfAllNull: Optional[Union[bool, str]] = None,\n",
    "        encoding: Optional[str] = None,\n",
    "        locale: Optional[str] = None,\n",
    "        pathGlobFilter: Optional[Union[bool, str]] = None,\n",
    "        recursiveFileLookup: Optional[Union[bool, str]] = None,\n",
    "        modifiedBefore: Optional[Union[bool, str]] = None,\n",
    "        modifiedAfter: Optional[Union[bool, str]] = None,\n",
    "        allowNonNumericNumbers: Optional[Union[bool, str]] = None,\n",
    "        useUnsafeRow: Optional[Union[bool, str]] = None,"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_read",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
