{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f1fc00-ba32-4343-94dd-578b10141902",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766388731993}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import when,col\n",
    "schema1=\"id int,name string,age int,salary decimal(10,2),join_date date,remarks string,corrupt_record string \"\n",
    "\n",
    "df1=spark.read.schema(schema1).csv(\"/Volumes/myowndatasets/malformed_data/malformed_csv/csv_malformeddata.csv\",header=True,quote='\"',mode=\"PERMISSIVE\",columnNameOfCorruptRecord=\"corrupt_record\")\n",
    "df2=df1.withColumn(\n",
    "    \"salary\",\n",
    "    when(col(\"salary\").isNull(), 0).otherwise(col(\"salary\"))\n",
    ")\n",
    "\n",
    "df3 = df2.withColumn(\n",
    "    \"join_date\",\n",
    "    when(col(\"id\") == 9, \"2022-09-01\").otherwise(col(\"join_date\"))\n",
    ")\n",
    "\n",
    "display(df3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "malformed_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
