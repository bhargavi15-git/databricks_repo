{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4ca97b-aa37-4f9f-9080-dc2734b20c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Versioning/History/TimeTravel\n",
    "\n",
    "\n",
    "1. **History **\n",
    "History is captured for every commit as a row in '**describe history**' command.. ie for each and every transaction(insert/update/delete). Even the ddl change and optimize is also captured as a row\n",
    "![image_1770570329414.png](./image_1770570329414.png \"image_1770570329414.png\")\n",
    "2. **Version**\n",
    "- **'version as of'** will display the respective snapshot version of the table\n",
    "- Usually 0 version is create table and it doesnt show any record.\n",
    "\n",
    "3. **Time Travel**\n",
    "- **Timestamp as of** Reads the table as it existed at that exact timestamp and Any commits after the given timestamp is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ac3528-e140-46a8-8273-7bdeca7d344c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770570772642}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY data_optimization.default.sampletable1;\n",
    "\n",
    "select * from data_optimization.default.sampletable1 timestamp as of '2026-02-05T14:51:25.000+00:00';\n",
    "select * from data_optimization.default.sampletable version as of 2; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91035c17-57a5-4fdb-b064-64bb698666e4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770573069668}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "describe data_optimization.data_db.drug_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc8be05-fd78-4581-8338-7766a304a1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop table if exists data_optimization.data_db.drug_tbl2;\n",
    "create table data_optimization.data_db.drug_tbl2(\n",
    "uniqueid int,\n",
    "drugname string,\n",
    "condition  string,\n",
    "rating int,\n",
    "date date,\n",
    "usefulcount int)\n",
    "using delta\n",
    "partitioned by(rating);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b176a0-a5be-493d-a614-067f3b5a4f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Optimize\n",
    "####1Ô∏è‚É£ Before OPTIMIZE ‚Äî baseline Delta table\n",
    "\n",
    "##### Directory structure\n",
    "\n",
    "```\n",
    "delta_lab/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000002.json\n",
    "‚îú‚îÄ‚îÄ part-00000-aaa.parquet\n",
    "‚îú‚îÄ‚îÄ part-00001-bbb.parquet\n",
    "‚îú‚îÄ‚îÄ part-00002-ccc.parquet\n",
    "```\n",
    "\n",
    "Assumption:\n",
    "\n",
    "* Small files\n",
    "* Some deletes/updates already happened\n",
    "* Deletion Vectors are enabled\n",
    "\n",
    "#### 2Ô∏è‚É£ What the Parquet files contain (conceptually)\n",
    "##### `part-00000-aaa.parquet`\n",
    "```\n",
    "id | name\n",
    "---------\n",
    "1  | A\n",
    "2  | B   (deleted later)\n",
    "3  | C\n",
    "```\n",
    "\n",
    "##### `part-00001-bbb.parquet`\n",
    "```\n",
    "id | name\n",
    "---------\n",
    "4  | D\n",
    "5  | E\n",
    "```\n",
    "\n",
    "##### Deletion Vector (DV)\n",
    "Stored separately (simplified):\n",
    "\n",
    "```\n",
    "DV for part-00000 ‚Üí row index {1}\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Row `(2, B)` is deleted\n",
    "* File still physically contains it\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Delta log BEFORE OPTIMIZE (important)\n",
    "\n",
    "##### Example `00000000000000000002.json`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-aaa.parquet\",\n",
    "    \"size\": 2048,\n",
    "    \"deletionVector\": {\n",
    "      \"storageType\": \"u\",\n",
    "      \"pathOrInlineDv\": \"dv-0001\",\n",
    "      \"cardinality\": 1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "‚û°Ô∏è File is **active**\n",
    "‚û°Ô∏è DV masks deleted rows\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ OPTIMIZE is triggered\n",
    "\n",
    "```sql\n",
    "OPTIMIZE delta_lab;\n",
    "```\n",
    "\n",
    "Now the magic happens.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ What OPTIMIZE actually DOES internally\n",
    "\n",
    "##### OPTIMIZE reads:\n",
    "\n",
    "* All **active Parquet files**\n",
    "* Their **deletion vectors**\n",
    "* Applies the **latest snapshot**\n",
    "\n",
    "##### OPTIMIZE writes:\n",
    "\n",
    "* **New Parquet files**\n",
    "* Containing **only live rows**\n",
    "* With **no deletion vectors**\n",
    "\n",
    "---\n",
    "\n",
    "#### 6Ô∏è‚É£ Directory structure AFTER OPTIMIZE\n",
    "\n",
    "```\n",
    "delta_lab/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000003.json   ‚Üê OPTIMIZE commit\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000003.checkpoint.parquet\n",
    "‚îú‚îÄ‚îÄ part-00000-aaa.parquet          ‚Üê old (logically removed)\n",
    "‚îú‚îÄ‚îÄ part-00001-bbb.parquet          ‚Üê old (logically removed)\n",
    "‚îú‚îÄ‚îÄ part-00002-ccc.parquet          ‚Üê old (logically removed)\n",
    "‚îú‚îÄ‚îÄ part-00000-zzz.parquet          ‚Üê NEW optimized file\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Old files still exist physically\n",
    "‚ö†Ô∏è They are no longer referenced\n",
    "\n",
    "---\n",
    "\n",
    "#### 7Ô∏è‚É£ Contents of the NEW optimized Parquet file\n",
    "\n",
    "##### `part-00000-zzz.parquet`\n",
    "\n",
    "```\n",
    "id | name\n",
    "---------\n",
    "1  | A\n",
    "3  | C\n",
    "4  | D\n",
    "5  | E\n",
    "```\n",
    "\n",
    "‚úÖ Deleted row `(2, B)` is **gone**\n",
    "‚úÖ No DV needed anymore\n",
    "\n",
    "---\n",
    "\n",
    "#### 8Ô∏è‚É£ Delta log entry CREATED by OPTIMIZE\n",
    "\n",
    "##### `00000000000000000003.json`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"operation\": \"OPTIMIZE\",\n",
    "    \"operationMetrics\": {\n",
    "      \"numRemovedFiles\": \"3\",\n",
    "      \"numAddedFiles\": \"1\",\n",
    "      \"numDeletedRows\": \"1\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-zzz.parquet\",\n",
    "    \"size\": 8192,\n",
    "    \"dataChange\": false\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"remove\": {\n",
    "    \"path\": \"part-00000-aaa.parquet\",\n",
    "    \"deletionTimestamp\": 1700000000000\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"remove\": {\n",
    "    \"path\": \"part-00001-bbb.parquet\",\n",
    "    \"deletionTimestamp\": 1700000000000\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"remove\": {\n",
    "    \"path\": \"part-00002-ccc.parquet\",\n",
    "    \"deletionTimestamp\": 1700000000000\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 9Ô∏è‚É£ Key observations (THIS IS THE GOLD)\n",
    "\n",
    "##### üîπ OPTIMIZE is snapshot-based\n",
    "\n",
    "* It does **not care** how many updates/deletes happened\n",
    "* It only materializes **final valid rows**\n",
    "\n",
    "##### üîπ OPTIMIZE removes DV indirectly\n",
    "\n",
    "* DV is **not copied**\n",
    "* Clean Parquet files are written\n",
    "\n",
    "##### üîπ History is still intact\n",
    "\n",
    "* Old files are only **logically removed**\n",
    "* Time travel still works\n",
    "\n",
    "---\n",
    "\n",
    "#### üîü What VACUUM does AFTER this\n",
    "\n",
    "```sql\n",
    "VACUUM delta_lab;\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```\n",
    "‚ùå part-00000-aaa.parquet\n",
    "‚ùå part-00001-bbb.parquet\n",
    "‚ùå part-00002-ccc.parquet\n",
    "```\n",
    "\n",
    "Only this remains:\n",
    "\n",
    "```\n",
    "delta_lab/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îú‚îÄ‚îÄ part-00000-zzz.parquet\n",
    "```\n",
    "\n",
    "\n",
    "#### üîë Ultra-clear mental model\n",
    "\n",
    "```\n",
    "DELETE / UPDATE ‚Üí DV or rewrite\n",
    "OPTIMIZE        ‚Üí snapshot materialization\n",
    "VACUUM          ‚Üí physical cleanup\n",
    "```\n",
    "\n",
    "#### Interview-level one-liner\n",
    "\n",
    "> **OPTIMIZE rewrites Delta data files based on the latest snapshot, compacting files and eliminating deleted rows and deletion vectors, while preserving history through logical remove entries in the transaction log.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609ceda5-6792-4f78-83f0-879388e95e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Vaccum:\n",
    "#### 1Ô∏è‚É£ State BEFORE VACUUM (post-OPTIMIZE)\n",
    "##### Directory structure\n",
    "\n",
    "```\n",
    "delta_lab/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000003.json   ‚Üê OPTIMIZE commit\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000003.checkpoint.parquet\n",
    "‚îú‚îÄ‚îÄ part-00000-aaa.parquet          ‚Üê old, logically removed\n",
    "‚îú‚îÄ‚îÄ part-00001-bbb.parquet          ‚Üê old, logically removed\n",
    "‚îú‚îÄ‚îÄ part-00002-ccc.parquet          ‚Üê old, logically removed\n",
    "‚îú‚îÄ‚îÄ part-00000-zzz.parquet          ‚Üê ACTIVE\n",
    "```\n",
    "Important:\n",
    "* Old files have `remove` entries in Delta log\n",
    "* Files still exist physically\n",
    "* Time travel still works\n",
    "\n",
    "\n",
    "#### 2Ô∏è‚É£ VACUUM is triggered\n",
    "\n",
    "```sql\n",
    "VACUUM delta_lab;\n",
    "```\n",
    "Default:\n",
    "```text\n",
    "RETAIN 168 HOURS (7 days)\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ What VACUUM actually READS\n",
    "\n",
    "VACUUM reads **only metadata**, not table data:\n",
    "\n",
    "##### Reads:\n",
    "\n",
    "* Latest Delta snapshot\n",
    "* `add` actions ‚Üí active files\n",
    "* `remove` actions ‚Üí deletion timestamps\n",
    "* Retention policy\n",
    "\n",
    "##### Builds two sets:\n",
    "\n",
    "```\n",
    "ACTIVE FILES    = { part-00000-zzz.parquet }\n",
    "REMOVED FILES   = {\n",
    "  part-00000-aaa.parquet,\n",
    "  part-00001-bbb.parquet,\n",
    "  part-00002-ccc.parquet\n",
    "}\n",
    "```\n",
    "\n",
    "#### 4Ô∏è‚É£ Retention check (critical)\n",
    "\n",
    "For each removed file:\n",
    "\n",
    "```\n",
    "current_time - deletionTimestamp >= retention\n",
    "```\n",
    "\n",
    "If TRUE ‚Üí eligible for deletion\n",
    "\n",
    "#### 5Ô∏è‚É£ What VACUUM DELETES (physically)\n",
    "\n",
    "##### Storage layer AFTER VACUUM\n",
    "\n",
    "```\n",
    "delta_lab/\n",
    "‚îú‚îÄ‚îÄ _delta_log/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000003.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000003.checkpoint.parquet\n",
    "‚îú‚îÄ‚îÄ part-00000-zzz.parquet          ‚Üê ACTIVE (kept)\n",
    "```\n",
    "\n",
    "‚ùå Old Parquet files are **gone**\n",
    "\n",
    "#### 6Ô∏è‚É£ What VACUUM does NOT touch (very important)\n",
    "\n",
    "##### Delta logs:\n",
    "\n",
    "* ‚ùå No new JSON files\n",
    "* ‚ùå No new checkpoint\n",
    "* ‚ùå No table version increment\n",
    "\n",
    "##### Metadata:\n",
    "\n",
    "* ‚ùå No add/remove entries\n",
    "* ‚ùå No history rewrite\n",
    "\n",
    "VACUUM is **read-only on `_delta_log`**.\n",
    "\n",
    "#### 7Ô∏è‚É£ Why current queries still work\n",
    "\n",
    "Because:\n",
    "* Current snapshot references only:\n",
    "\n",
    "  ```\n",
    "  part-00000-zzz.parquet\n",
    "  ```\n",
    "* Spark never looks for deleted files\n",
    "\n",
    "#### 8Ô∏è‚É£ What BREAKS after VACUUM\n",
    "\n",
    "##### Time travel beyond retention:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM delta_lab VERSION AS OF 2;\n",
    "```\n",
    "\n",
    "‚ùå Fails with:\n",
    "\n",
    "```\n",
    "FileNotFoundException\n",
    "```\n",
    "\n",
    "Because:\n",
    "\n",
    "* Delta log references files\n",
    "* Files no longer exist physically\n",
    "\n",
    "---\n",
    "\n",
    "#### 9Ô∏è‚É£ VACUUM with deletion vectors (DV case)\n",
    "\n",
    "If DVs existed earlier:\n",
    "\n",
    "* Old DV files referenced by removed files\n",
    "* Become unreferenced\n",
    "* VACUUM deletes DV files too\n",
    "\n",
    "But:\n",
    "\n",
    "* DV for active files (if any) are preserved\n",
    "\n",
    "---\n",
    "\n",
    "#### üîü Full lifecycle diagram\n",
    "\n",
    "```\n",
    "INSERT / UPDATE / DELETE\n",
    "        ‚Üì\n",
    "Delta log add/remove\n",
    "        ‚Üì\n",
    "OPTIMIZE\n",
    "        ‚Üì\n",
    "New files + remove old files (logical)\n",
    "        ‚Üì\n",
    "VACUUM\n",
    "        ‚Üì\n",
    "Physical deletion of old files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîë Ultra-important mental model\n",
    "\n",
    "> **OPTIMIZE changes the logical snapshot.\n",
    "> VACUUM changes only physical storage.**\n",
    "\n",
    "They NEVER overlap responsibilities.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interview-ready one-liner\n",
    "\n",
    "> **VACUUM reads Delta transaction logs to identify obsolete files and physically deletes them from storage after the retention period, without modifying table metadata or history.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc29f205-6d71-49f6-86ec-88f6d40626da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VACUUM delta_lab;\n",
    "--default retention is 168 hrs(7 days)\n",
    "VACUUM delta_lab RETAIN 2 HOURS;\n",
    "ALTER TABLE table_name SET TBLPROPERTIES ('delta.deletedFileRetentionDuration' = '24 hours');\n",
    "--If you attempt to run VACUUM with a retention period lower than 168 hours (7 days), Databricks will throw an error to prevent accidental data loss. To override this, \n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "VACUUM table_name RETAIN 1 HOURS;\n",
    "\n",
    "\n",
    "--DRY RUN \n",
    "VACUUM table_name RETAIN 24 HOURS DRY RUN;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "824420a3-7f56-45a5-ac10-0b6b26e3d7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ACID \n",
    "ACID Transactions\n",
    "**Delta Lake supports ACID transactions under the hood via a transaction log.**\n",
    "| ACID        | In Databricks         |\n",
    "| ----------- | --------------------- |\n",
    "| Atomicity   | Every transactions are Individual Transactions / All or nothing.A transaction is an indivisible unit. Either all its operations are executed successfully, or none are, preventing partial updates. If one part fails, the entire transaction is rolled back.  |\n",
    "| Consistency | Schema + constraints.A transaction must transform the database from one valid state to another, maintaining all predefined rules, constraints, and integrity checks.  |\n",
    "| Isolation   | Using Version/Time/restore we can isolate transactions, we can't use TCL (commit/rollback).Concurrent transactions do not interfere with each other. Each transaction behaves as if it is the only one operating on the data, preventing issues like dirty reads or inconsistent data.Isolation guarantees that concurrent transactions do not see each other‚Äôs intermediate or partial changes. |\n",
    "| Durability  | Every transaction Always hit the disk (durable), but can be controlled by Transaction log,Once a transaction is committed, its changes are permanently saved in the database, surviving any subsequent system failures.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefac511-7a55-41e4-b300-e40aaa000e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Atomicity and consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde30a92-8b2b-4269-9f15-6a32b419a729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Atomicity:\n",
    "--From start transaction until commit everything is one transaction. But in databricks every single statement is a transaction and it is autocommitted.\n",
    "--eg:\n",
    "INSERT INTO table_name VALUES (1,'a'),(2,'b');\n",
    "--This is a transaction and it is autocommitted.\n",
    "\n",
    "\n",
    "--Consistency:\n",
    "--Define nulls or constraints for a table\n",
    "--eg: \n",
    "CREATE TABLE abc (\n",
    "id int not null,   --not null constraint\n",
    "name string default unknown,\n",
    "age int check age>18   --age validation constraint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5ed4814-4da8-4c13-a932-1748c2ec244f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Isloation:\n",
    "Classic problems Isolation prevents<br>\n",
    "**Reading garbage data:**<br>\n",
    "Txn A: UPDATE orders SET amount = 1000 WHERE id = 1;\n",
    "       (not committed yet)\n",
    "<br>\n",
    "Txn B: SELECT amount FROM orders WHERE id = 1;\n",
    "       ‚Üí sees 1000 ‚ùå<br>\n",
    "If Txn A fails ‚Üí Txn B read garbage.<br>\n",
    "**with Delta:**\n",
    "- Txn B reads the last committed snapshot\n",
    "- Uncommitted files are invisible<br>\n",
    "\n",
    "**Non-repeatable Read (data changes mid-query):**<br>\n",
    "**Without isolation (bad)**<br>\n",
    "Txn A: SELECT salary FROM emp WHERE id = 10; ‚Üí 50000<br>\n",
    "Txn B: UPDATE emp SET salary = 60000 WHERE id = 10; COMMIT;<br>\n",
    "Txn A: SELECT salary FROM emp WHERE id = 10; ‚Üí 60000 ‚ùå<br>\n",
    "Same query, different result.<br>\n",
    "**With Delta isolation (good)**<br>\n",
    "Txn A: SELECT salary FROM emp WHERE id = 10; ‚Üí 50000<br>\n",
    "Txn B: UPDATE emp SET salary = 60000 WHERE id = 10; COMMIT;<br>\n",
    "Txn A: SELECT salary FROM emp WHERE id = 10; ‚Üí 50000 ‚úÖ<br>\n",
    "Why?\n",
    "Txn A keeps reading the same snapshot.Changes by Txn B are visible only after Txn A ends\n",
    "\n",
    "**Phantom Read (rows appear/disappear)**<br>\n",
    "**Without isolation (bad)<br>**\n",
    "Txn A: SELECT COUNT(*) FROM orders WHERE region = 'US'; ‚Üí 10<br>\n",
    "Txn B: INSERT INTO orders VALUES (..., 'US'); COMMIT;<br>\n",
    "Txn A: SELECT COUNT(*) FROM orders WHERE region = 'US'; ‚Üí 11 ‚ùå<br>\n",
    "**With Delta isolation (good)**<br>\n",
    "Txn A: SELECT COUNT(*) FROM orders WHERE region = 'US'; ‚Üí 10<br>\n",
    "Txn B: INSERT INTO orders VALUES (..., 'US'); COMMIT;<br>\n",
    "Txn A: SELECT COUNT(*) FROM orders WHERE region = 'US'; ‚Üí 10 ‚úÖ<br>\n",
    "**Delta snapshot isolation ensures:**<br>\n",
    "No new rows ‚Äúappear‚Äù mid-transaction<br>\n",
    "\n",
    "**Lost Update (very important)**<br>\n",
    "**Without isolation (bad)**<br>\n",
    "Initial balance = 100<br>\n",
    "Txn A: balance = balance - 30  ‚Üí writes 70<br>\n",
    "Txn B: balance = balance - 50  ‚Üí writes 50<br>\n",
    "Final result = 50 ‚ùå<br>\n",
    "One update is lost.<br>\n",
    "\n",
    "**With Delta isolation (good)**<br>\n",
    "Txn A commits first<br>\n",
    "Txn B tries to commit ‚Üí CONFLICT ‚ùå<br>\n",
    "\n",
    "**Delta detects:**\n",
    "Both touched the same rows/files\n",
    "Second writer must retry\n",
    "\n",
    "![image_1770642173766.png](./image_1770642173766.png \"image_1770642173766.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9e255e-ae85-4f75-94ae-f0d11475e07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Durability\n",
    "\n",
    "**Delta uses two durable layers:**<br>\n",
    "\n",
    "- Immutable data files (Parquet)\n",
    "- Transaction log (_delta_log)\n",
    "\n",
    "Both are written to reliable storage (DBFS / cloud object storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2473e7-5166-4fff-a564-8a9b268d1d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#zorder\n",
    "Z-ORDER is a data layout optimization technique in Delta Lake that:\n",
    "- Rewrites data files\n",
    "- Physically clusters related column values together\n",
    "- Reduces the number of files and row groups scanned during queries\n",
    "\n",
    "**note: Zorder can be done only manually using optmize..zorder command**\n",
    "\n",
    "Imagine a Delta table with files like this:\n",
    "part-0001 ‚Üí ids: 1‚Äì1M (random ids)<br>\n",
    "part-0002 ‚Üí ids: 1‚Äì1M (random ids)<br>\n",
    "part-0003 ‚Üí ids: 1‚Äì1M (random ids)<br>\n",
    "\n",
    "what spark does without zorder:\n",
    "SELECT * FROM orders WHERE customer_id = 101;\n",
    "- ‚ùå Spark scans many files\n",
    "- ‚ùå Poor data skipping\n",
    "- ‚ùå High IO\n",
    "\n",
    "**OPTIMIZE table_name ZORDER BY (col1, col2, ...);**\n",
    "\n",
    "What Z-ORDER does (high level)\n",
    "- Takes selected columns\n",
    "- Applies Z-curve (Morton ordering)\n",
    "- Rewrites Parquet files so similar values live together\n",
    "\n",
    "part-0001 ‚Üí customer_id: 1‚Äì1000<br>\n",
    "part-0002 ‚Üí customer_id: 1001‚Äì2000<br>\n",
    "part-0003 ‚Üí customer_id: 2001‚Äì3000\n",
    "Now Spark:\n",
    "- Reads fewer files\n",
    "- Uses min/max stats efficiently\n",
    "- Skips unrelated data\n",
    "\n",
    "What happens when zorder:\n",
    "- optimize starts\n",
    "- Takes active parquet files,ignores inactive files,applies deletion vetor\n",
    "- z-values is computed for the given columns\n",
    "-Larger files (‚âà 1GB default) with physically clustered rows(Old small files still exist (for now),later deleted during vaccum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2115f47e-3f37-4060-b996-642e3baf3b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Can zorder handledata skew?\n",
    "Z-ORDER is not a primary solution for data skew, but it can reduce the impact of skew in some read scenarios.\n",
    "\n",
    "- Now let‚Äôs break that down properly.\n",
    "- What ‚Äúdata skew‚Äù really means (important)\n",
    "\n",
    "There are two very different skews people mix up:\n",
    "**Query / shuffle skew:**\n",
    "- One key ‚Üí huge amount of data\n",
    "- One task runs forever\n",
    "- Others finish fast\n",
    "\n",
    "**Storage / file-level skew**:\n",
    "- Some files contain most of the relevant rows\n",
    "- Others are rarely read\n",
    "\n",
    "Z-ORDER only helps with #2, not #1.\n",
    "\n",
    "Can Z-ORDER fix shuffle skew?‚ùå No<br>\n",
    "\n",
    "Example:<br>\n",
    "SELECT *\n",
    "FROM orders\n",
    "GROUP BY customer_id;\n",
    "\n",
    "If:\n",
    "customer_id = 1 ‚Üí 60% of rows\n",
    "\n",
    "\n",
    "Even after Z-ORDER:\n",
    "- All those rows still hash to one reducer\n",
    "- One task still does most of the work\n",
    "- üëâ Skew remains\n",
    "- Where Z-ORDER does help\n",
    "- ‚úÖ Filter skew (read-side skew)\n",
    "\n",
    "Query:\n",
    "SELECT *\n",
    "FROM orders\n",
    "WHERE customer_id = 1;\n",
    "\n",
    "**Without Z-ORDER:**\n",
    "- Data spread across many files\n",
    "- Many tasks launched\n",
    "- Lots of IO\n",
    "\n",
    "**With Z-ORDER:**\n",
    "- Rows for customer_id = 1 clustered\n",
    "- Fewer files read\n",
    "- Fewer tasks launched\n",
    "- üëâ Less wasted work\n",
    "\n",
    "**‚úÖ Join-side scan skew (partial help)**<br>\n",
    "SELECT *\n",
    "FROM orders o\n",
    "JOIN customers c\n",
    "ON o.customer_id = c.customer_id\n",
    "WHERE c.region = 'EU';\n",
    "\n",
    "\n",
    "Z-ORDER on customer_id:\n",
    "- Orders for EU customers are in fewer files\n",
    "- Spark scans less data before shuffle\n",
    "- üëâ Shuffle skew still exists, but input size is smaller\n",
    "\n",
    "What Z-ORDER does NOT do\n",
    "\n",
    "- ‚ùå Does not split hot keys\n",
    "- ‚ùå Does not rebalance partitions\n",
    "- ‚ùå Does not change hash distribution\n",
    "- ‚ùå Does not fix long-running tasks\n",
    "\n",
    "Correct tools for data skew (this is key)\n",
    "| Problem                   | Correct solution  |\n",
    "| ------------------------- | ----------------- |\n",
    "| Hot keys in joins         | Salting           |\n",
    "| Large vs small table join | Broadcast join    |\n",
    "| Skewed aggregations       | AQE skew handling |\n",
    "| Write skew                | Repartition       |\n",
    "| Small files               | OPTIMIZE          |\n",
    "| Read locality             | Z-ORDER           |\n",
    "\n",
    "Z-ORDER vs real skew solutions\n",
    "| Tool        | Fixes skew? | How                      |\n",
    "| ----------- | ----------- | ------------------------ |\n",
    "| Z-ORDER     | ‚ö†Ô∏è Partial  | Reduces files scanned    |\n",
    "| Repartition | ‚úÖ           | Redistributes data       |\n",
    "| Salting     | ‚úÖ           | Breaks hot keys          |\n",
    "| AQE         | ‚úÖ           | Splits skewed partitions |\n",
    "| Broadcast   | ‚úÖ           | Removes shuffle          |\n",
    "\n",
    "\n",
    "Databricks themselves position Z-ORDER as:\n",
    "**a read-optimization technique, not a skew-handling mechanism**\n",
    "\n",
    "**Z-ORDER does not fix data skew, but it can reduce its impact by limiting how much skewed data is scanned during reads.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7301c16f-e9d9-4fc5-bdbb-ab59b022c8ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Liquid Clustering\n",
    "Liquid Clustering is a dynamic, self-managing clustering mechanism for Delta tables that:\n",
    "- Continuously reorganizes data\n",
    "- Adapts automatically as data changes\n",
    "- Eliminates the need for manual OPTIMIZE ZORDER\n",
    "- Think of it as:\n",
    "- ‚ÄúZ-ORDER that keeps fixing itself over time.‚Äù\n",
    "\n",
    "**Z-ORDER problems:**\n",
    "- Static (needs manual runs)\n",
    "- Expensive full rewrites\n",
    "- Degrades as new data arrives\n",
    "- Needs careful column selection\n",
    "- Liquid Clustering fixes all of that.\n",
    "\n",
    "**How Liquid Clustering works (internals)**\n",
    "- **You define clustering columns (once)**<br>\n",
    "CREATE TABLE orders (<br>\n",
    "  cust_name STRING,<br>\n",
    "  cust_id BIGINT,<br>\n",
    ")\n",
    "USING DELTA<br>\n",
    "CLUSTER BY (customer_id, order_date);<br>\n",
    "No partitions required.\n",
    "\n",
    "- **Data is written normally**\n",
    "-     Streaming or batch\n",
    "-     Inserts, updates, deletes\n",
    "-     Deletion vectors supported\n",
    "-     No immediate reordering.\n",
    "\n",
    "- **Databricks monitors clustering quality**\n",
    "-     Internally tracks:\n",
    "-     File overlap\n",
    "-     Range dispersion\n",
    "-     Query access patterns\n",
    "\n",
    "- **Incremental re-clustering happens**\n",
    "-     During:OPTIMIZE orders;\n",
    "-     Only badly clustered files are rewritten\n",
    "-     Not the entire table\n",
    "-     Small, incremental rewrites\n",
    "\n",
    "- **Query-time benefits**\n",
    "-     Strong data skipping\n",
    "-     Fewer files scanned\n",
    "-     Stable performance over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f8ffdc-95b7-4af3-91e0-2a847978909d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Liquid Clustering vs Z-ORDER\n",
    "| Aspect                  | Z-ORDER        | Liquid Clustering |\n",
    "| ----------------------- | -------------- | ----------------- |\n",
    "| Configuration           | Manual per run | Defined once      |\n",
    "| Rewrite scope           | Full optimize  | Incremental       |\n",
    "| Handles frequent writes | ‚ùå Poorly       | ‚úÖ Excellent       |\n",
    "| Streaming-friendly      | ‚ùå              | ‚úÖ                 |\n",
    "| Maintenance cost        | High           | Low               |\n",
    "| Column count            | 1‚Äì4            | 1‚Äì4               |\n",
    "| Adapts over time        | ‚ùå              | ‚úÖ                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d09c793-b6f5-4525-88a3-51741c8e5bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Partition By\n",
    "\n",
    "Note: Exactly works like hive.. create folders and creates part files inside it\n",
    "\n",
    "CREATE OR REPLACE TABLE customer_txn_part1 ( <br>\n",
    "    txn_id INT, \n",
    "    customer_id INT,\n",
    "    txn_amount DOUBLE,\n",
    "    transaction_date DATE\n",
    ") <br>\n",
    "using delta<br>\n",
    "partitioned by (transaction_date);<br>\n",
    "insert into customer_txn_part1 select * from customer_txn;<br>\n",
    "\n",
    "- show partitions customer_txn_part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c12dfbd-cfbb-4dfb-80d7-27a6d622fdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#CTAS VS DEEP CLONE VS SHALLOW CLONE\n",
    "\n",
    "| Aspect                         | **CTAS**<br>(Create Table As Select)   | **Deep Clone**                      | **Shallow Clone**                  |\n",
    "| ------------------------------ | -------------------------------------- | ----------------------------------- | ---------------------------------- |\n",
    "| Purpose                        | Create a new table from a query result | Full physical copy of a Delta table | Logical copy referencing same data |\n",
    "| Copies data files              | ‚úÖ Yes (new Parquet files)              | ‚úÖ Yes (full data copy)              | ‚ùå No                               |\n",
    "| Copies metadata                | ‚ùå Partial (schema only from SELECT)    | ‚úÖ Yes                               | ‚úÖ Yes                              |\n",
    "| Copies table history           | ‚ùå No                                   | ‚ùå No                                | ‚ùå No                               |\n",
    "| Copies constraints             | ‚ùå No                                   | ‚úÖ Yes                               | ‚úÖ Yes                              |\n",
    "| Copies table properties        | ‚ùå No                                   | ‚úÖ Yes                               | ‚úÖ Yes                              |\n",
    "| Copies Z-ORDER / clustering    | ‚ùå No                                   | ‚úÖ Yes                               | ‚úÖ Yes                              |\n",
    "| Storage usage                  | High                                   | Very high                           | Very low                           |\n",
    "| Performance after creation     | Depends on SELECT                      | Same as source                      | Same as source                     |\n",
    "| Data independence              | Fully independent                      | Fully independent                   | ‚ùå Not independent                  |\n",
    "| Underlying files shared        | ‚ùå No                                   | ‚ùå No                                | ‚úÖ Yes                              |\n",
    "| Time to create                 | Medium‚ÄìSlow                            | Slow                                | Very fast                          |\n",
    "| Incremental sync possible      | ‚ùå No                                   | ‚ùå No                                | ‚ùå No                               |\n",
    "| Supports Unity Catalog         | ‚úÖ Yes                                  | ‚úÖ Yes                               | ‚úÖ Yes                              |\n",
    "| Supports Time Travel           | ‚ùå Fresh table only                     | ‚úÖ From clone creation               | ‚úÖ From clone creation              |\n",
    "| Affected if source VACUUM runs | ‚ùå No                                   | ‚ùå No                                | ‚úÖ Yes                              |\n",
    "| Best for                       | Transformations, aggregations          | Backup, migration                   | Dev/Test, experiments              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfc1d5e-912a-4932-bdea-05b7c6288ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##CTAS\n",
    "- Query runs\n",
    "- New Parquet files are written\n",
    "- New Delta log starts at version 0\n",
    "- No relationship to source table<br>\n",
    "What is NOT copied:\n",
    "- ‚ùå History\n",
    "- ‚ùå Constraints\n",
    "- ‚ùå Table properties\n",
    "- ‚ùå Z-ORDER / clustering<br>\n",
    "**Syntax:**<br>\n",
    "CREATE TABLE sales_ctas<br>\n",
    "AS<br>\n",
    "SELECT * FROM sales WHERE region = 'EU';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85340072-0940-4183-92aa-2526c843c5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Deep Clone:\n",
    "What actually happens<br>\n",
    "- All active data files are copied\n",
    "- Metadata is copied\n",
    "- New Delta log is created\n",
    "- Source and target are fully independent\n",
    "\n",
    "What is NOT copied<br>\n",
    "- ‚ùå Transaction history\n",
    "- ‚ùå Old versions\n",
    "\n",
    "Storage impact<br>\n",
    "- üí∏ Doubles storage immediately\n",
    "\n",
    "Syntax:\n",
    "CREATE TABLE sales_deep_clone\n",
    "CLONE sales;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab059bfb-0a03-4541-b6c9-f93e11f2b970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Shallow clone:\n",
    "What actually happens\n",
    "- No data files copied\n",
    "- Delta log references source files\n",
    "- Reads are redirected to original files\n",
    "\n",
    "Source table files<br>\n",
    "        ‚ñ≤<br>\n",
    "        ‚îÇ<br>\n",
    "Shallow clone metadata\n",
    "\n",
    "\n",
    "Very important behavior:\n",
    "- Source table UPDATE/DELETE:\n",
    "- Does NOT affect clone (new files written)\n",
    "\n",
    "VACUUM on source:\n",
    "- ‚ùå Can break clone if retention mismanaged\n",
    "\n",
    "Storage impact\n",
    "- üí∞ Almost free\n",
    "\n",
    "Syntax:\n",
    "CREATE TABLE sales_deep_clone\n",
    "SHALLOW CLONE sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2f08a1-d251-4796-96e5-ccfbcee2c388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop table if exists base_table;\n",
    "CREATE TABLE  if not exists base_table(\n",
    "  id int,\n",
    "  name string,\n",
    "  dept string\n",
    ")\n",
    "using delta\n",
    "PARTITIONED BY (dept);\n",
    "insert  into  base_table values(1,'a','maths'),(2,'b','science'),(3,'c','maths');\n",
    "update base_table set dept='physics' where id=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9922897e-43a1-4f4a-8cd8-79b5f9d1e974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "drop table if exists base_table_ctas;\n",
    "CREATE TABLE IF NOT EXISTS base_table_ctas as select * from  base_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294ada29-8d80-492f-8873-d4cbc6d6bbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY base_table_ctas;\n",
    "--no partition/cluster columns was preserved\n",
    "--no history/ version was preserved\n",
    "-- just queried table and wrote results into a new file and metadata started from version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cfd1af-02e4-4a21-bad4-85b3adaccb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS \n",
    "base_table_deep_clone clone base_table;\n",
    "DESCRIBE HISTORY base_table_deep_clone;\n",
    "--partition/cluster columns was preserved\n",
    "--Deep Clone preserves data state, not data history.\n",
    "--History/version reset to 0\n",
    "-- Deep clone just copies all active files related to current base table snapshot and removes all the deletion vectors and inactive files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33afa42b-c62d-40ce-aae4-5964fb5be0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop table if exists base_table_shallow_clone;\n",
    "CREATE TABLE base_table_shallow_clone SHALLOW CLONE base_table;\n",
    "DESCRIBE HISTORY base_table_shallow_clone;\n",
    "--partition/cluster columns was preserved\n",
    "select * from base_table_shallow_clone version as of 1;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_optimizations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
