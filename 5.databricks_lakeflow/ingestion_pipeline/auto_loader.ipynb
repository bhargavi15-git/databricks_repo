{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6247a36b-323f-4464-ad79-75a1c668e184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Autoloader\n",
    "**Auto Loader is Databricks‚Äô cloud-native file ingestion engine for ingesting new files incrementally from object storage.**\n",
    "\n",
    "Supported Sources:\n",
    "- AWS S3\n",
    "- Azure ADLS Gen2\n",
    "- Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6a17d9-3fa3-44e0-b49f-643a6048df42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Types of Autoloader:\n",
    "**1. Directory Listing/Triggered / Incremental Batch Pipeline (Micro-batch)**\n",
    "- Runs Auto Loader in streaming mode but triggers once (or on schedule).<br>\n",
    "üîπ Use case:\n",
    "- Daily/hourly ingestion\n",
    "- Orchestrated by Databricks Jobs\n",
    "- Cost-optimized ingestion\n",
    "\n",
    "**What really happens:**\n",
    "- Databricks job gets t**riggered based on the schedule**\n",
    "- It **looks for the new files** in the given cloud storage location(validates with checkpoints and gets only new files)\n",
    "- Copy the files.. **validates the schema**(with schema from the previous load). If there is any **change in the schema(updates the schema location**).\n",
    "- **Loads the file** into the target path with mergeschema feature(incase of schema change)\n",
    "- **Post loading the checkpoint location** is loaded with the list of processed files\n",
    "\n",
    "\n",
    "**2. Continuous (Streaming) Ingestion Pipeline**\n",
    "- Auto Loader** runs in Structured Streaming mode and continuously monitors cloud storage** for new files.<br>\n",
    "üîπ Use case:\n",
    "- Near real-time ingestion\n",
    "- IoT data\n",
    "- Application logs\n",
    "- Event-based files landing continuously\n",
    "\n",
    "What really happends:\n",
    "1. Cloud storage emits file-create event (S3 Event, ADLS Event Grid, GCS Pub/Sub)\n",
    "2. Event is delivered to Databricks queue\n",
    "3. Auto Loader receives notification (push model)\n",
    "4. New file is registered\n",
    "5. Infers schema / evolves if needed\n",
    "6. Copy the file(s) & store the schema info in a schema file, so further schema inference is not needed.\n",
    "7. Updates checkpoint (file1 is processed...)\n",
    "8. Stream stays idle until next event arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c732f0f4-e63b-4937-af2e-497c5cde7dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Benefits of Autoloader:\n",
    "1. Meant **only for file ingestion from cloud object storage**(gcs/s3/adls)\n",
    "2. **Supports streaming(streaming pipeline) and scheduled jobs(Direcotry listing)**\n",
    "3. **Incremental and efficient file tracking:** with checkpoint automates ingestion of only new data\n",
    "4. **Schema evlotion support:** with \"cloudFiles.schemaEvolutionMode\": \"addNewColumns\" and \"mergeSchema\": \"true\" .. adds columns without breaking the pipeline\n",
    "5. **Scalability and Resource Optimization**:Properties such as \"cloudFiles.maxFilesPerTrigger\" allow you to control how many files are processed per batch\n",
    "6. **Checkpointing and Fault Tolerance:** with checkpointing.. incase of job failures it can resume from the point where it failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa47aac4-5573-4679-be19-b108d27019f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Readstream important options:\n",
    "\n",
    "- cloudFiles.format\n",
    "- cloudFiles.schemaLocation\n",
    "- checkpointLocation\n",
    "- cloudFiles.inferColumnTypes\n",
    "- cloudFiles.schemaEvolutionMode\n",
    "- rescuedDataColumn\n",
    "- cloudFiles.includeExistingFiles\n",
    "- cloudFiles.maxFilesPerTrigger\n",
    "- cloudFiles.partitionColumns\n",
    "- cloudFiles.validateOptions\n",
    "\n",
    "\n",
    "| Category            | Option                            | Description                                                             |\n",
    "| ------------------- | --------------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Required**        | `cloudFiles.format`               | Source file format (csv, json, parquet, avro, etc.)                     |\n",
    "| **Required**        | `cloudFiles.schemaLocation`       | Location to store schema metadata and evolution history                 |\n",
    "| **Schema Handling** | `cloudFiles.inferColumnTypes`     | Enables automatic data type inference                                   |\n",
    "| **Schema Handling** | `cloudFiles.schemaEvolutionMode`  | Controls schema changes (`addNewColumns`, `rescue`, `failOnNewColumns`) |\n",
    "| **Schema Handling** | `rescuedDataColumn`               | Column to capture unexpected or malformed fields                        |                                |\n",
    "| **File Discovery**  | `cloudFiles.includeExistingFiles` | Whether to process historical files at first run                        |\n",
    "| **File Discovery**  | `cloudFiles.maxFilesPerTrigger`   | Maximum files processed per micro-batch                                 |\n",
    "| **Performance**     | `cloudFiles.partitionColumns`     | Extract partition columns from directory structure                      |\n",
    "| **Performance**     | `cloudFiles.validateOptions`      | Validates provided Auto Loader options                                  |\n",
    "| **Performance**     | `cloudFiles.backfillInterval`     | Frequency of directory backfill when using notifications(Event based)                |\n",
    "| **File Discovery**  | `cloudFiles.useNotifications`     | Enables file notification mode (event-based ingestion)                  |\n",
    "| **File Discovery**  | `cloudFiles.queueName`            | Cloud queue used for file notifications(event-based ingestion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faaeb6be-1563-4371-9018-a8b77f824a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#writestream options:\n",
    "\n",
    "| Option                         | Description                       |\n",
    "| ------------------------------ | --------------------------------- |\n",
    "| `format()`                     | delta, parquet, console, memory   |\n",
    "| `outputMode()`                 | append(Only new rows (most common)), complete(Full aggregation results), update(Updated rows only)          |\n",
    "| `option(\"checkpointLocation\")` | Required for fault tolerance      |\n",
    "| `path`                         | Output path (if not using table)  |\n",
    "| `partitionBy()`                | Partition output files            |\n",
    "| `queryName()`                  | Name the streaming query          |\n",
    "| `mergeSchema`                  | Enable schema evolution for Delta |\n",
    "| `truncate`                     | For console sink                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c7262c-d13b-4337-a244-ab37b1f3733c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Trigger options:\n",
    "Why triggers comes with writestream but not readstream.<br>\n",
    "Because readstream is just definition:<br>\n",
    "**readStream Defines:**\n",
    "- Where to read from\n",
    "- Schema\n",
    "- Source options\n",
    "\n",
    "** It does NOT:**\n",
    "- Execute anything\n",
    "- Schedule anything\n",
    "- Create batches\n",
    "\n",
    "**writeStream Defines:**\n",
    "- Output sink\n",
    "- Output mode\n",
    "- Checkpoint\n",
    "- Trigger\n",
    "- Starts the stream\n",
    "- This is where the engine runs.\n",
    "\n",
    "| Trigger Type                    | Syntax                                  | Runs Continuously? | Stops Automatically?                        | Needs Job Scheduling?              | Typical Use Case                              |\n",
    "| ------------------------------- | --------------------------------------- | ------------------ | ------------------------------------------- | ---------------------------------- | --------------------------------------------- |\n",
    "| **Micro-Batch Continuous**      | `.trigger(processingTime=\"30 seconds\")` | ‚úÖ Yes              | ‚ùå No                                        | ‚ùå No (runs as long-running job)    | Near real-time ingestion                      |\n",
    "| **Trigger Once (Old)**          | `.trigger(once=True)`                   | ‚ùå No               | ‚úÖ Yes (after one batch)                     | ‚úÖ Yes (if periodic runs needed)    | Legacy batch-style streaming                  |\n",
    "| **Available Now (Recommended)** | `.trigger(availableNow=True)`           | ‚ùå No               | ‚úÖ Yes (after processing all available data) | ‚úÖ Yes (for hourly/daily pipelines) | Scheduled ingestion / streaming-as-batch      |\n",
    "| **Continuous Mode (Rare)**      | `.trigger(continuous=\"1 second\")`       | ‚úÖ Yes              | ‚ùå No                                        | ‚ùå No (runs continuously)           | Ultra-low latency streaming (limited support) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1bb35f-17ba-4d8d-94ed-28371997b95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create catalog if not exists auto_loader;\n",
    "create schema if not exists auto_loader.auto_loader_sch;\n",
    "create volume if not exists auto_loader.auto_loader_sch.auto_load_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197ff271-5b53-424e-81f3-d6088de7cfff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "cloudscrpath='/Volumes/auto_loader/auto_loader_sch/auto_load_vol/cloudsrc/'\n",
    "chkpointlocation='/Volumes/auto_loader/auto_loader_sch/auto_load_vol/checkpointlocation/'\n",
    "schemalocation='/Volumes/auto_loader/auto_loader_sch/auto_load_vol/schemalocation/'\n",
    "bronzetgt=\"/Volumes/auto_loader/auto_loader_sch/auto_load_vol/bronzetgt/\"\n",
    "df1=spark.readStream.format(\"cloudFiles\")\\\n",
    ".option(\"cloudFiles.format\",\"csv\")\\\n",
    ".option(\"cloudFiles.inferColumnTypes\",True)\\\n",
    ".option(\"header\",True)\\\n",
    ".option(\"cloudFiles.schemaEvolutionMode\",\"addNewColumns\")\\\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\",5)\\\n",
    ".option(\"cloudFiles.schemaLocation\", schemalocation)\\\n",
    ".option(\"checkpointLocation\", chkpointlocation)\\\n",
    ".load(cloudscrpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a493ba-6f28-4608-8eb9-77600e1e80aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df1.writeStream.trigger(availableNow=True)\\\n",
    ".format(\"delta\")\\\n",
    ".option(\"checkpointLocation\", chkpointlocation)\\\n",
    ".option(\"cloudFiles.schemaLocation\", schemalocation)\\\n",
    ".option(\"mergeSchema\", \"true\") \\\n",
    ".start(bronzetgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56497eab-cb2b-4ef4-bd25-e9ea9191bbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.read.format(\"delta\").load('/Volumes/auto_loader/auto_loader_sch/auto_load_vol/bronzetgt/').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "auto_loader",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
