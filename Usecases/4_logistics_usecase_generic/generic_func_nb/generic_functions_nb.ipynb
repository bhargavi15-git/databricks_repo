{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1096a4-3430-4796-8487-8558140cff83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "#Return Spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(app_name=\"spark_app_1\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (SparkSession.builder.appName(app_name).getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b49be7-bc59-4f80-8cb3-9f8d9075291b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def read_file(file_type:str='csv',path:str=None,header:bool=True,inferschema:bool=True,sep:str=None,linesep=None,schema:str=None,recursivefilelookup:bool=True,pathglobalfilter:str=None,modifiedbefore=None,modifiedafter=None,mode:str='PERMISSIVE',dateformat:str='yyyy-MM-dd',timestampformat:str='yyyy-MM-dd HH:mm:ss',mergeschema:str=True,multiline:str=False,table:str=None,is_malformed=False):\n",
    "\n",
    "\n",
    "        \n",
    "    if  file_type=='csv':\n",
    "        return spark.read.options(header=header,inferSchema=inferschema,sep=sep,lineSep=linesep,recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter,mode=mode,dateFormat=dateformat,timestampFormat=timestampformat,schema=schema).csv(path)\n",
    "    elif file_type=='json':\n",
    "        return spark.read.options(recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,multiLine=multiline,mode=mode,dateFormat=dateformat,timestampFormat=timestampformat,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter,schema=schema).json(path)\n",
    "    elif file_type=='orc' or file_type=='parquet' or file_type=='delta':\n",
    "        return spark.read.options(recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,mergeSchema=mergeschema,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter).format(file_type).load(path)\n",
    "    elif file_type=='table':\n",
    "        return spark.read.table(table)\n",
    "    else:\n",
    "        print('file type not supported')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5f32bd-7307-4dc8-a6e6-361181045124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def write_file(df,file_type:str='delta',path:str=None,mode:str='overwrite',table:str=None):\n",
    "    if file_type in ['csv','json','orc','parquet','delta']:\n",
    "        return df.write.mode(mode).format(file_type).save(path)\n",
    "    elif file_type=='table':\n",
    "        return df.write.mode(mode).saveAsTable(table)\n",
    "    else:\n",
    "        print('file type not supported')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd78ae57-82ed-4cd6-94c6-d58da5886482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def merge_df(df1,df2,allowmissingcolumns=True):\n",
    "  return df1.unionByName(df2,allowMissingColumns=allowmissingcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9baa57-db53-4f94-9cce-3bcbd47108bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lit,col\n",
    "def add_column_with_default(df,column_name,default_value):\n",
    "  return df.withColumn(f'{column_name}',lit(f'{default_value}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32311f73-eb4c-4f9d-b544-0ad19e325493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def cleansing_func(df,duplicatedatacolumns:list=None,nulldropcolumns:list=[],nullstatergy='any'):\n",
    "  df1=df.distinct()\n",
    "  df2=df1.dropDuplicates(duplicatedatacolumns)\n",
    "  df3=df2.dropna(how=nullstatergy,subset=nulldropcolumns)\n",
    "  return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfc70d8-65fc-4d59-91eb-8b232b0071be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install word2number\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from word2number import w2n\n",
    "\n",
    "def word_to_num(value):\n",
    "    try:\n",
    "        # If already numeric\n",
    "        return int(value)\n",
    "    except:\n",
    "        try:\n",
    "            return w2n.word_to_num(value.lower())\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "word_to_num_udf = udf(word_to_num, IntegerType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb92c217-3e45-4737-af8d-c6c7a8edb8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_temp_view(df,tempviewname):\n",
    "    return df.createOrReplaceTempView(tempviewname)\n",
    "\n",
    "def df_from_temp_view(tempviewname):\n",
    "    return spark.sql(f'select * from {tempviewname}')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generic_functions_nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
