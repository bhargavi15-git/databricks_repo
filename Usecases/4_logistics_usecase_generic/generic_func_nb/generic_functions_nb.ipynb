{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1096a4-3430-4796-8487-8558140cff83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "def create_spark_session(appname:str='new_session'):\n",
    "    return SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b49be7-bc59-4f80-8cb3-9f8d9075291b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def read_file(file_type:str='csv',path:str=None,header:bool=True,inferschema:bool=True,sep:str=None,linesep=None,schema:str=None,recursivefilelookup:bool=True,pathglobalfilter:str=None,modifiedbefore=None,modifiedafter=None,mode:str='PERMISSIVE',dateformat:str='yyyy-MM-dd',timestampformat:str='yyyy-MM-dd HH:mm:ss',mergeschema:str=True,multiline:str=False,table:str=None,is_malformed=False):\n",
    "\n",
    "\n",
    "        \n",
    "    if  file_type=='csv':\n",
    "        return spark.read.options(header=header,inferSchema=inferschema,sep=sep,lineSep=linesep,recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter,mode=mode,dateFormat=dateformat,timestampFormat=timestampformat,schema=schema).csv(path)\n",
    "    elif file_type=='json':\n",
    "        return spark.read.options(recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,multiLine=multiline,mode=mode,dateFormat=dateformat,timestampFormat=timestampformat,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter,schema=schema).json(path)\n",
    "    elif file_type=='orc' or file_type=='parquet' or file_type=='delta':\n",
    "        return spark.read.options(recursiveFileLookup=recursivefilelookup,pathGlobFilter=pathglobalfilter,mergeSchema=mergeschema,modifiedBefore=modifiedbefore,modifiedAfter=modifiedafter).format(file_type).load(path)\n",
    "    elif file_type=='table':\n",
    "        return spark.read.table(table)\n",
    "    else:\n",
    "        print('file type not supported')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5f32bd-7307-4dc8-a6e6-361181045124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def write_file(df,file_type:str='delta',path:str=None,mode:str='overwrite',table:str=None):\n",
    "    if file_type in ['csv','json','orc','parquet','delta']:\n",
    "        return df.write.mode(mode).format(file_type).save(path)\n",
    "    elif file_type=='table':\n",
    "        return df.write.mode(mode).saveAsTable(table)\n",
    "    else:\n",
    "        print('file type not supported')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd78ae57-82ed-4cd6-94c6-d58da5886482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def merge_df(df1,df2,allowmissingcolumns=True):\n",
    "  return df1.unionByName(df2,allowMissingColumns=allowmissingcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb9baa57-db53-4f94-9cce-3bcbd47108bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lit,col\n",
    "def add_column_with_default(df,column_name,default_value):\n",
    "  return df.withColumn(f'{column_name}',lit(f'{default_value}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32311f73-eb4c-4f9d-b544-0ad19e325493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def cleansed_data(df,duplicatedatacolumns:list=None,nulldropcolumns:list=[],nullstatergy='any'):\n",
    "  df1=df.distinct()\n",
    "  df2=df1.dropDuplicates(duplicatedatacolumns)\n",
    "  df3=df2.dropna(how=nullstatergy,subset=nulldropcolumns)\n",
    "  return df3"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generic_functions_nb",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
