{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb0cfa4-ac9e-427c-af7c-3eae56f173ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Logistics source1 file:\n",
    "- Format- CSV\n",
    "- delimiter - comma\n",
    "- line sep - new line\n",
    "- header in file - yes\n",
    "- missing columns\n",
    "- extra columns\n",
    "- data for format issues- id and age(expected int but received string)\n",
    "- empty rows\n",
    "\n",
    "Logistics source2 file:\n",
    "\n",
    "- Format- CSV\n",
    "- delimiter - comma\n",
    "- line sep - new line\n",
    "- header in file - yes\n",
    "- missing columns\n",
    "- data for format issues- id and age(expected int but received string)\n",
    "- empty rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a394761-8826-4286-bbb6-fcd72a453e7b",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769492836729}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp_1\n",
    "(\n",
    "shipment_id INT,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age INT,\n",
    "role STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source1\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\"\n",
    "\n",
    ");\n",
    "select * from shipment_temp_1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a03ab97-e0cb-4910-aed8-9bab7c2bc680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Use describe instead of schema or columns or dtypes or schema to understand the column names with datatypes\n",
    "display(spark.sql(\"describe shipment_temp_1\"))\n",
    "shipment_df1=spark.sql(\"select * from shipment_temp_1\")\n",
    "\n",
    "##Understanding duplicates for individual columns\n",
    "display(spark.sql(\"select count(*) from shipment_temp_1\"))\n",
    "display(spark.sql(\"select count(distinct shipment_id) from shipment_temp_1\"))\n",
    "display(shipment_df1.dropDuplicates([\"shipment_id\"]).count())\n",
    "display(spark.sql(\"select count(distinct first_name) from shipment_temp_1\"))\n",
    "display(shipment_df1.dropDuplicates([\"first_name\"]).count())\n",
    "display(spark.sql(\"select count(distinct last_name) from shipment_temp_1\"))\n",
    "display(shipment_df1.dropDuplicates([\"last_name\"]).count())\n",
    "\n",
    "##Understanding duplicates for combination of columns is not possible becase only the first column with distinct will be considered\n",
    "display(spark.sql(\"select count(*) from shipment_temp_1\"))\n",
    "##Below code doesnt work it applies distinct only on shipment_id\n",
    "display(spark.sql(\"select count(distinct shipment_id, first_name, last_name) from shipment_temp_1\"))\n",
    "display(shipment_df1.dropDuplicates([\"shipment_id\",\"first_name\",\"last_name\"]).count())\n",
    "\n",
    "##Understanding duplicate row is not possible directly\n",
    "display(spark.sql(\"select count(*) from shipment_temp_1\"))\n",
    "\n",
    "##below code doesnt work as expected\n",
    "display(spark.sql(\"select count(distinct *) from shipment_temp_1\"))\n",
    "display(shipment_df1.dropDuplicates().count())\n",
    "\n",
    "#summary or describe helps us to understand statistical data understanding\n",
    "#it shows there are nulls in few columns values, avg, stddev, min, max, percentile distribution\n",
    "#to achive this we need to agg funtions But summary or describe can give this detail very easily\n",
    "\n",
    "display(spark.sql(\"select count(shipment_id) as count,avg(shipment_id) avg, stddev(shipment_id), min(shipment_id), max(shipment_id) from shipment_temp_1\"))\n",
    "\n",
    "shipment_df1.summary().show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"MY Spark Session\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd18ae9f-575b-4e66-b479-dd5170bbcaad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp_2\n",
    "(\n",
    "shipment_id INT,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age INT,\n",
    "role STRING,\n",
    "corrupt_record STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source1\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\",\n",
    "  columnNameOfCorruptRecord \"corrupt_record\"\n",
    "\n",
    ");\n",
    "--Identifying the rows with less columns and additional columns\n",
    "select * from shipment_temp_2 where corrupt_record is NOT NULL;\n",
    "\n",
    "--identifying non int age from logistics_source1\n",
    "--Here we dont have any string age boz.. while reading the file we read age as int\n",
    "\n",
    "--INcase we read it as string and let all values.. direct casting is not possible .. we need to convert all strings manually to int using update\n",
    "\n",
    "/*UPDATE person\n",
    "SET age =\n",
    "  CASE age\n",
    "    WHEN 'one' THEN '1'\n",
    "    WHEN 'two' THEN '2'\n",
    "    WHEN 'three' THEN '3'\n",
    "    WHEN 'ten' THEN '10'\n",
    "    ELSE age\n",
    "  END;*/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b346d5aa-8869-463e-ae1f-3e0d699f1226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp1\n",
    "(\n",
    "shipment_id INT,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age INT,\n",
    "role STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source1\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\"\n",
    "\n",
    ");\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp2\n",
    "(\n",
    "shipment_id INT,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age INT,\n",
    "role STRING,\n",
    "hub_location STRING,\n",
    "vehicle_type STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source2\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52a6816-d5ee-4229-aff8-43f07152fc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT t1.shipment_id FROM  shipment_temp1 t1 join shipment_temp2 t2 on t1.shipment_id = t2.shipment_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e0d3a6-542b-4b6b-bc79-e49e6ada9a8d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769500874951}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp_1\n",
    "(\n",
    "shipment_id STRING,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age STRING,\n",
    "\n",
    "role STRING,\n",
    "hub_location STRING,\n",
    "vehicle_type STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source1\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\"\n",
    ");\n",
    "CREATE  OR REPLACE TEMPORARY VIEW shipment_temp_2\n",
    "(\n",
    "shipment_id STRING,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "age STRING,\n",
    "role STRING,\n",
    "hub_location STRING,\n",
    "vehicle_type STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_source2\",\n",
    "  header  \"true\",\n",
    "  inferSchema  \"false\"\n",
    "\n",
    ");\n",
    "\n",
    "Create or replace temp view shipment_temp_raw as select *,'source1' as data_source from shipment_temp_1  union all select  *,'source2' as data_source from shipment_temp_2 ;\n",
    "drop table if exists usecase_data.logistics_sql_data.shipment_raw_table;\n",
    "create table if not exists usecase_data.logistics_sql_data.shipment_raw_table as select * from shipment_temp_raw;\n",
    "select * from shipment_temp_raw;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461211c4-291c-43e6-ad5c-193a0f112df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW master_city_list( \n",
    "city_name STRING,\n",
    "country STRING,\n",
    "latitude DECIMAL(10,6),\n",
    "longitude DECIMAL(10,6))\n",
    "USING CSV\n",
    "OPTIONS(\n",
    "header \"True\",\n",
    "path \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/Master_City_List.csv\",\n",
    "inferSchema \"False\"\n",
    ");\n",
    "DROP table IF EXISTS usecase_data.logistics_sql_data.master_city_table;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.master_city_table AS SELECT * FROM master_city_list;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9c8e04-4285-44c9-a271-0149037a7407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view shipment_temp_raw1\n",
    "as \n",
    "select \n",
    "shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "Case when coalesce(age,'-1') = 'ten' or coalesce(age,'-1') = '' then '-1'  else age end as age,\n",
    "role,\n",
    "case when hub_location = 'Additionalcolumn' then NULL else hub_location end as hub_location,\n",
    "coalesce(vehicle_type,'UNKNOWN') AS vehicle_type,\n",
    "data_source\n",
    "from shipment_temp_raw \n",
    "where shipment_id is not null and \n",
    "role is not null \n",
    "and ( first_name is not null or last_name is not null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db413639-d4ed-401d-aedd-b9caab079c08",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769523121149}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW  logistics_shipment_temp \n",
    "(shipment_id INT,\n",
    "order_id STRING,\n",
    "source_city STRING,\n",
    "destination_city STRING,\n",
    "shipment_status STRING,\n",
    "cargo_type STRING,\n",
    "vehicle_type STRING,\n",
    "payment_mode STRING,\n",
    "shipment_weight_kg FLOAT,\n",
    "shipment_cost FLOAT,\n",
    "shipment_date STRING)\n",
    "USING JSON\n",
    "OPTIONS\n",
    "(\n",
    "  path  \"dbfs:/Volumes/usecase_data/logistics_proj_data/projdata/logistics_shipment_detail_3000.json\",\n",
    "  multiLine \"True\");\n",
    "  DROP TABLE IF EXISTS  usecase_data.logistics_sql_data.logistics_shipment_raw_table;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.logistics_shipment_raw_table AS SELECT * FROM logistics_shipment_temp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a38459-4b7b-4589-9285-ab1c4601cf85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW  logistics_shipment_temp_1\n",
    "AS SELECT\n",
    "shipment_id ,\n",
    "order_id ,\n",
    "source_city ,\n",
    "destination_city ,\n",
    "shipment_status ,\n",
    "cargo_type ,\n",
    "upper(vehicle_type) AS vehicle_type ,\n",
    "payment_mode ,\n",
    "round(cast(shipment_weight_kg as double),2) as shipment_weight_kg,\n",
    "round(cast(shipment_cost as double),2) as shipment_cost ,\n",
    "to_date(shipment_date,'yy-MM-dd') as shipment_date,\n",
    "'Logistics' AS domain, \n",
    "current_timestamp() as  ingestion_timestamp,\n",
    "False as is_expedited\n",
    "FROM logistics_shipment_temp;\n",
    "--select * from  logistics_shipment_temp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e670adf-64ab-4d67-bba7-0550281f9436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW shipment_temp_raw2\n",
    "AS SELECT \n",
    "cast(shipment_id as int)as shipment_id,\n",
    "first_name as staff_first_name,\n",
    "last_name as staff_last_name,\n",
    "cast(age as int) as age,\n",
    "LOWER(role) AS role,\n",
    "initcap(hub_location) as origin_hub_city,\n",
    "vehicle_type,\n",
    "data_source\n",
    " FROM shipment_temp_raw1 where shipment_id != 'ten';\n",
    "select * from shipment_temp_raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75758d0-c0d9-4e79-8a3c-cfe96c7928f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--row level distinct\n",
    "create or replace temporary view shipment_temp_raw3 as select distinct * from shipment_temp_raw2;\n",
    "--column level distinct\n",
    "create or replace temporary view shipment_temp_raw4 as  select * from shipment_temp_raw3 \n",
    "qualify row_number() over(partition by shipment_id order by data_source asc)=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03e1ca7-5440-4907-ab76-1bc8ac07dcde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temporary view shipment_temp_raw5\n",
    "as select \n",
    "shipment_id,\n",
    "trim(concat_ws('',staff_first_name,' ',staff_last_name)) as full_name,\n",
    "age,\n",
    "role,\n",
    "origin_hub_city,\n",
    "vehicle_type,\n",
    "data_source,\n",
    "current_timestamp() as load_dt\n",
    "from shipment_temp_raw4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday.\n",
    "\n",
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f244c23f-ee43-4f60-a210-ba6ab81f224a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temporary view logistics_shipment_temp_2 as\n",
    "select \n",
    "shipment_id,\n",
    "order_id ,\n",
    "source_city ,\n",
    "destination_city ,\n",
    "shipment_status ,\n",
    "cargo_type ,\n",
    "vehicle_type ,\n",
    "payment_mode ,\n",
    "shipment_weight_kg,\n",
    "shipment_cost ,\n",
    "shipment_date,\n",
    "domain, \n",
    "ingestion_timestamp,\n",
    "case when shipment_status in ('IN_TRANSIT','DELIVERED') then True else False end as is_expedited,\n",
    "concat_ws('',source_city,'-',destination_city) as route_segment,\n",
    "concat_ws('',vehicle_type,'_',shipment_id) as vehicle_identifier,\n",
    "year(shipment_date) as shipment_year,\n",
    "month(shipment_date) as shipment_month,\n",
    "case when \n",
    "dayofweek(shipment_date) in(1,7) then True else False end as is_weekend,\n",
    "round(try_divide(shipment_cost,shipment_weight_kg),2) as cost_per_kg,\n",
    "datediff(current_date(), shipment_date) as days_since_shipment,\n",
    "round(shipment_cost * 0.18,2) as tax_amount,\n",
    "substring(order_id,1,3) as order_prefix,\n",
    "substring(order_id,4,len(order_id)) as order_sequence,\n",
    "year(shipment_date) as ship_year,\n",
    "month(shipment_date) as ship_month,\n",
    "day(shipment_date) as ship_day,\n",
    "concat_ws('',source_city,'->',destination_city) as route_lane,\n",
    "case when shipment_cost > 50000 then True else False end as is_high_value\n",
    "from logistics_shipment_temp_1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffef412-5f5e-4640-b4a0-58f9bdd5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temporary view shipment_temp_raw6\n",
    "as \n",
    "select\n",
    "shipment_id,\n",
    "concat(substring(full_name,1,2),repeat('*',len(full_name)-3),substring(full_name,-1)) as staff_full_name,\n",
    "age,\n",
    "role,\n",
    "origin_hub_city,\n",
    "vehicle_type,\n",
    "data_source,\n",
    "current_timestamp() as load_dt,\n",
    "case \n",
    "when role='driver' and age > 50 then 0.15\n",
    "when role='driver' and age < 30 then 0.05\n",
    "else 0\n",
    "end as projected_bonus\n",
    "from shipment_temp_raw5;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"₹30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" → **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `priority_flag` (Descending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd539cc-ec20-4acc-8189-be46fa416c1f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1769537670768}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.driver_app_data;\n",
    "\n",
    "CREATE Table if not EXISTS \n",
    "usecase_data.logistics_sql_data.driver_app_data\n",
    "as \n",
    "select staff_full_name, role, origin_hub_city from shipment_temp_raw6;\n",
    "\n",
    "DROP TABLE IF EXISTS usecase_data.logistics_sql_data.active_operational_problem_data;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.active_operational_problem_data\n",
    "as select * from logistics_shipment_temp_2 where shipment_status IN ('DELAYED','RETURNED');\n",
    "\n",
    "drop table if exists usecase_data.logistics_sql_data.senior_insurance_audit;\n",
    "\n",
    "CREATE Table if not EXISTS \n",
    "usecase_data.logistics_sql_data.senior_insurance_audit\n",
    "as \n",
    "select * from shipment_temp_raw6 where age >50;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7969b40f-ccae-4dc0-9477-01a119aae865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW logistics_shipment_temp_3 AS\n",
    "select \n",
    "shipment_id as log_shipment_id,\n",
    "order_id ,\n",
    "upper(source_city) as source_city ,\n",
    "destination_city ,\n",
    "shipment_status ,\n",
    "cargo_type ,\n",
    "vehicle_type as shipment_vehicle_type,\n",
    "payment_mode ,\n",
    "shipment_weight_kg,\n",
    "shipment_cost,\n",
    "concat('₹',cast(shipment_cost as string)) as shipment_cost_inr ,\n",
    "shipment_date,\n",
    "domain, \n",
    "ingestion_timestamp,\n",
    "is_expedited,\n",
    "route_segment,\n",
    "vehicle_identifier,\n",
    "shipment_year,\n",
    "shipment_month,\n",
    "is_weekend,\n",
    "cost_per_kg,\n",
    "days_since_shipment,\n",
    "tax_amount,\n",
    "order_prefix,\n",
    "order_sequence,\n",
    "ship_year,\n",
    "ship_month,\n",
    "ship_day,\n",
    "route_lane,\n",
    "is_high_value\n",
    "from logistics_shipment_temp_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0e71e2-5461-4be8-8a94-9ff656b70cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.regional_staffing_analysis;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.regional_staffing_analysis as \n",
    "select origin_hub_city, count(*) as staff_count from shipment_temp_raw6 group by origin_hub_city;\n",
    "\n",
    "drop table if exists usecase_data.logistics_sql_data.fleet_capacity_analysis;\n",
    "    \n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.fleet_capacity_analysis as \n",
    "select shipment_vehicle_type, sum(shipment_weight_kg) as total_weight from logistics_shipment_temp_3 group by shipment_vehicle_type;\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b0510f-8072-4058-893e-77787333d639",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769539226860}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--daily dispatch schedule based on the cost priority\n",
    "drop table if exists usecase_data.logistics_sql_data.daily_dispatch_schedule;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.daily_dispatch_schedule AS \n",
    "SELECT\n",
    "* FROM logistics_shipment_temp_3 order by shipment_date asc, shipment_cost desc;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45eb7976-011d-45c0-b8a4-b088050439f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.critical_delays;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.critical_delays as\n",
    "select * from  logistics_shipment_temp_3 where shipment_status='DELAYED' order by shipment_date asc,shipment_cost desc limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519c6f6b-681e-4f0c-a533-6f0db21ee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.staff_with_shipments;\n",
    "CREATE TABLE if not exists usecase_data.logistics_sql_data.staff_with_shipments AS\n",
    "SELECT\n",
    "*\n",
    "from shipment_temp_raw6 t1 \n",
    "inner join logistics_shipment_temp_3 t2 \n",
    "on t1.shipment_id=t2.log_shipment_id;\n",
    "\n",
    "drop table if exists usecase_data.logistics_sql_data.idle_staff;\n",
    "CREATE TABLE if not exists usecase_data.logistics_sql_data.idle_staff\n",
    "as \n",
    "select t1.* \n",
    "from shipment_temp_raw6 t1 \n",
    "left join logistics_shipment_temp_3 t2 \n",
    "on t1.shipment_id=t2.log_shipment_id \n",
    "where t2.log_shipment_id is null;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671ecd46-fcbe-45fa-9719-7c3a620567ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select t1.shipment_id,t1.origin_hub_city,t2.shipment_id,t2.origin_hub_city from shipment_temp_raw6 t1 join shipment_temp_raw6 t2 on t1.origin_hub_city=t2.origin_hub_city where t1.shipment_id!=t2.shipment_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9b0a4e-dfeb-4471-a4ad-671e3d128e89",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769583546077}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.invalid_driver_to_shipments;\n",
    "CREATE TABLE if not exists usecase_data.logistics_sql_data.invalid_driver_to_shipments\n",
    "as \n",
    "select t2.* from shipment_temp_raw6 t1 right join logistics_shipment_temp_3 t2 on t1.shipment_id=t2.log_shipment_id where t1.shipment_id is null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6fbcbb-7ba8-4e93-9de6-599a53f14328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from shipment_temp_raw6 t1 full outer join logistics_shipment_temp_3 t2 on t1.shipment_id=t2.log_shipment_id where t1.shipment_id is null or t2.log_shipment_id is null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9ca34e-9b84-4d23-9c42-3e036a9dd380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH driver_tl1 AS (\n",
    "    SELECT *\n",
    "    FROM shipment_temp_raw6\n",
    "    WHERE role = 'driver'\n",
    "),\n",
    "pending_shipment AS (\n",
    "    SELECT *\n",
    "    FROM logistics_shipment_temp_3\n",
    "    WHERE shipment_status = 'DELAYED'\n",
    ")\n",
    "SELECT *\n",
    "FROM driver_tl1 t1\n",
    "JOIN pending_shipment t2\n",
    "ON 1 = 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fc6205-4ac7-4976-8cb0-7188676da48a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769585751003}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--left semi join\n",
    "select * from shipment_temp_raw6 t1 left semi join logistics_shipment_temp_3 t2 on t1.shipment_id=t2.log_shipment_id;\n",
    "--left anti join\n",
    "select * from shipment_temp_raw6 t1 left anti join logistics_shipment_temp_3 t2 on t1.shipment_id=t2.log_shipment_id;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de715e4c-174f-411f-9a04-914bcd559b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select t1.origin_hub_city from shipment_temp_raw6 t1 left semi join master_city_list t2 on t1.origin_hub_city=t2.city_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e23814-deee-42c4-9ab4-1855c8c9dfd1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769586645166}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS usecase_data.logistics_sql_data.geo_tagged_staff_data;\n",
    "CREATE TABLE IF NOT EXISTS usecase_data.logistics_sql_data.geo_tagged_staff_data AS\n",
    "select t1.*,t2.latitude,t2.longitude from shipment_temp_raw6 t1 left join master_city_list t2 on t1.origin_hub_city=t2.city_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861a35ad-4900-454b-8c87-1fab8796e561",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769607184754}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.wide_shipment_history;\n",
    "create table if not exists usecase_data.logistics_sql_data.wide_shipment_history as\n",
    "select * from logistics_shipment_temp_3 t1 left join shipment_temp_raw6 t2 on t1.log_shipment_id=t2.shipment_id left join master_city_list t3 on t2.origin_hub_city=t3.city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b877ece-277c-4b4c-8481-a64832f7b10f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769608601030}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.top_driver_list;\n",
    "create table if not exists usecase_data.logistics_sql_data.top_driver_list as\n",
    "with driver_list as (\n",
    "select * from usecase_data.logistics_sql_data.wide_shipment_history where role='driver')\n",
    "select * from driver_list qualify row_number() over(partition by origin_hub_city order by shipment_cost desc) <=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a0413f-1530-4353-a201-7de0328090f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *,date_diff(shipment_date,(lag(shipment_date) over(partition by log_shipment_id order by shipment_date asc))) as elapsed_days from logistics_shipment_temp_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ac463d-4e0d-4de1-aebb-adb0d5728583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.migrant_staff;\n",
    "create table if not exists usecase_data.logistics_sql_data.migrant_staff as\n",
    "select shipment_id from shipment_temp_1 intersect select shipment_id from shipment_temp_2;\n",
    "drop table if exists usecase_data.logistics_sql_data.new_hires;\n",
    "create table if not exists usecase_data.logistics_sql_data.new_hires as\n",
    "select shipment_id from shipment_temp_2 except select shipment_id from shipment_temp_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bdf309-c56b-4ce7-9b5a-1283277f81e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists usecase_data.logistics_sql_data.multi_level_report;\n",
    "create table if not exists usecase_data.logistics_sql_data.multi_level_report as\n",
    "select sum(shipment_cost) as total_cost, origin_hub_city, vehicle_type from usecase_data.logistics_sql_data.staff_with_shipments group by cube(origin_hub_city,vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3bd958-1784-4dac-9634-b415ea05fbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "multilevel_subtotal_report=inner_joined_df.cube(\"origin_hub_city\",\"vehicle_type\").agg(sum(\"shipment_cost\").alias(\"total_cost\"))\n",
    "display(multilevel_subtotal_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8880135484805418,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
